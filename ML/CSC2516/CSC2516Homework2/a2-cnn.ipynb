{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"“a2-cnn.ipynb”的副本","provenance":[{"file_id":"11sH_zV08QvCAYrGDv83lI9-5mnmln3SV","timestamp":1582041760169},{"file_id":"1OoD3hlsmDgO6OqtRScqJsIGxl1SSSJrr","timestamp":1581465514010},{"file_id":"1M4n5Tt0zwePLBfBZZaWWK5PlSu5jBl7E","timestamp":1580926613304}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JyzOT64xkqy6","colab_type":"text"},"source":["# Programming Assignment 2: Convolutional Neural Networks\n","Based on an assignment by Lisa Zhang\n","\n","For CSC413/2516 in Winter 2020 with Professor Jimmy Ba\n","\n","**Submission:**\n","You must submit two files through [MarkUs](https://markus.teach.cs.toronto.edu/csc413-2020-01): a PDF file containing your writeup, titled *a2-writeup.pdf*, and your code file *a2-cnn.ipynb*. Your writeup must be typeset.\n","\n","The programming assignments are individual work. See the [Course Syllabus](https://csc413-2020.github.io/assets/misc/syllabus.pdf) for detailed policies. \n","\n","**Introduction:**  \n","This assignment will focus on the applications of convolutional neural networks in various image processing tasks. First, we will train a convolutional neural network for a task known as image colourization. Given a greyscale image, we will predict the colour at each pixel. This a difficult problem for many reasons, one of which being that it is ill-posed: for a single greyscale image, there can be multiple,  equally valid colourings. \n","\n","In the second half of the assignment, we will perform fine-tuning on a pre-trained semantic segmentation model. Semantic segmentation attempts to clusters the areas of an image which belongs to the same object (label), and treats each pixel as a classification problem. We will fine-tune a pre-trained conv net featuring dilated convolution to segment flowers from the [Oxford17 flower dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/)"]},{"cell_type":"markdown","metadata":{"id":"TjPTaRB4mpCd","colab_type":"text"},"source":["# Colab FAQ and Using GPU\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the Colab GPU for this assignment by selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"cell_type":"markdown","metadata":{"id":"s9IS9B9-yUU5","colab_type":"text"},"source":["# Download CIFAR and Colour dictionary\n","We will use the [CIFAR-10 data set](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz), which consists of images of size 32x32 pixels. For most of the questions we will use a subset of the dataset. To make the problem easier, we will only use the “Horse” category from this data set. Now let’s learn to colour some horses!\n","\n","The data loading script is included below. It can take up to a couple of minutes to download everything the first time.\n","\n","All files are stored at `/content/csc413/a2/data/` folder"]},{"cell_type":"markdown","metadata":{"id":"4BIpGwANoQOg","colab_type":"text"},"source":["#### Helper code\n","You can ignore the restart warning."]},{"cell_type":"code","metadata":{"id":"piDmAsqFG0gU","colab_type":"code","colab":{}},"source":["######################################################################\n","# Setup working directory\n","######################################################################\n","%mkdir -p /content/csc413/a2/\n","%cd /content/csc413/a2\n","\n","######################################################################\n","# Helper functions for loading data\n","######################################################################\n","# adapted from \n","# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n","\n","import os\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import numpy as np\n","import pickle\n","import sys\n","from PIL import Image\n","\n","\n","def get_file(fname,\n","             origin,\n","             untar=False,\n","             extract=False,\n","             archive_format='auto',\n","             cache_dir='data'):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + '.tar.gz'\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","    \n","    print('File path: %s' % fpath)\n","    if not os.path.exists(fpath):\n","        print('Downloading data from', origin)\n","\n","        error_msg = 'URL fetch failure on {}: {} -- {}'\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print('Extracting file.')\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","def load_batch(fpath, label_key='labels'):\n","    \"\"\"Internal utility for parsing CIFAR data.\n","    # Arguments\n","        fpath: path the file to parse.\n","        label_key: key for label data in the retrieve\n","            dictionary.\n","    # Returns\n","        A tuple `(data, labels)`.\n","    \"\"\"\n","    f = open(fpath, 'rb')\n","    if sys.version_info < (3,):\n","        d = pickle.load(f)\n","    else:\n","        d = pickle.load(f, encoding='bytes')\n","        # decode utf8\n","        d_decoded = {}\n","        for k, v in d.items():\n","            d_decoded[k.decode('utf8')] = v\n","        d = d_decoded\n","    f.close()\n","    data = d['data']\n","    labels = d[label_key]\n","\n","    data = data.reshape(data.shape[0], 3, 32, 32)\n","    return data, labels\n","\n","def load_cifar10(transpose=False):\n","    \"\"\"Loads CIFAR10 dataset.\n","    # Returns\n","        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n","    \"\"\"\n","    dirname = 'cifar-10-batches-py'\n","    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n","    path = get_file(dirname, origin=origin, untar=True)\n","\n","    num_train_samples = 50000\n","\n","    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype='uint8')\n","    y_train = np.zeros((num_train_samples,), dtype='uint8')\n","\n","    for i in range(1, 6):\n","        fpath = os.path.join(path, 'data_batch_' + str(i))\n","        data, labels = load_batch(fpath)\n","        x_train[(i - 1) * 10000: i * 10000, :, :, :] = data\n","        y_train[(i - 1) * 10000: i * 10000] = labels\n","\n","    fpath = os.path.join(path, 'test_batch')\n","    x_test, y_test = load_batch(fpath)\n","\n","    y_train = np.reshape(y_train, (len(y_train), 1))\n","    y_test = np.reshape(y_test, (len(y_test), 1))\n","\n","    if transpose:\n","        x_train = x_train.transpose(0, 2, 3, 1)\n","        x_test = x_test.transpose(0, 2, 3, 1)\n","    return (x_train, y_train), (x_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jIKvJNtoVgU","colab_type":"text"},"source":["#### Download files\n","\n","This may take 1 or 2 mins for the first time."]},{"cell_type":"code","metadata":{"id":"l7fti3cryStt","colab_type":"code","colab":{}},"source":["# Download cluster centers for k-means over colours\n","colours_fpath = get_file(fname='colours', \n","                         origin='http://www.cs.toronto.edu/~jba/kmeans_colour_a2.tar.gz', \n","                         untar=True)\n","# Download CIFAR dataset\n","m = load_cifar10()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s1JvfupQm6F-","colab_type":"text"},"source":["# Part A. Colourization as Classification (2 pts)\n","\n","We will select a subset of 24 colours and frame colourization as a pixel-wise classification problem, where we label each pixel with one of 24 colours. The 24 colours are selected using [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) over colours, and selecting cluster centers. \n","\n","This was already done for you, and cluster centers are provided in http://www.cs.toronto.edu/~jba/kmeans_colour_a2.tar.gz, which was downloaded by the helper functions above. For simplicity, we will measure distance in RGB space. This is not ideal but reduces the software dependencies for this assignment."]},{"cell_type":"markdown","metadata":{"id":"oWyZwl9VKkxD","colab_type":"text"},"source":["## Helper code"]},{"cell_type":"code","metadata":{"id":"bTF1TQObE6DG","colab_type":"code","colab":{}},"source":["\"\"\"\n","Colourization of CIFAR-10 Horses via classification.\n","\"\"\"\n","from __future__ import print_function\n","import argparse\n","import math\n","import numpy as np\n","import numpy.random as npr\n","import scipy.misc\n","import time\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import matplotlib\n","import matplotlib.pyplot as plt \n","\n","\n","#from load_data import load_cifar10\n","\n","HORSE_CATEGORY = 7"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zN3FF8hIlS7h","colab_type":"text"},"source":["#### Data related code"]},{"cell_type":"code","metadata":{"id":"-900ROTMlSPd","colab_type":"code","colab":{}},"source":["def get_rgb_cat(xs, colours):\n","    \"\"\"\n","    Get colour categories given RGB values. This function doesn't\n","    actually do the work, instead it splits the work into smaller\n","    chunks that can fit into memory, and calls helper function\n","    _get_rgb_cat\n","\n","    Args:\n","      xs: float numpy array of RGB images in [B, C, H, W] format\n","      colours: numpy array of colour categories and their RGB values\n","    Returns:\n","      result: int numpy array of shape [B, 1, H, W]\n","    \"\"\"\n","    if np.shape(xs)[0] < 100:\n","        return _get_rgb_cat(xs)\n","    batch_size = 100\n","    nexts = []\n","    for i in range(0, np.shape(xs)[0], batch_size):\n","        next = _get_rgb_cat(xs[i:i+batch_size,:,:,:], colours)\n","        nexts.append(next)\n","    result = np.concatenate(nexts, axis=0)\n","    return result\n","\n","def _get_rgb_cat(xs, colours):\n","    \"\"\"\n","    Get colour categories given RGB values. This is done by choosing\n","    the colour in `colours` that is the closest (in RGB space) to\n","    each point in the image `xs`. This function is a little memory\n","    intensive, and so the size of `xs` should not be too large.\n","\n","    Args:\n","      xs: float numpy array of RGB images in [B, C, H, W] format\n","      colours: numpy array of colour categories and their RGB values\n","    Returns:\n","      result: int numpy array of shape [B, 1, H, W]\n","    \"\"\"\n","    num_colours = np.shape(colours)[0]\n","    xs = np.expand_dims(xs, 0)\n","    cs = np.reshape(colours, [num_colours,1,3,1,1])\n","    dists = np.linalg.norm(xs-cs, axis=2) # 2 = colour axis\n","    cat = np.argmin(dists, axis=0)\n","    cat = np.expand_dims(cat, axis=1)\n","    return cat\n","\n","def get_cat_rgb(cats, colours):\n","    \"\"\"\n","    Get RGB colours given the colour categories\n","\n","    Args:\n","      cats: integer numpy array of colour categories\n","      colours: numpy array of colour categories and their RGB values\n","    Returns:\n","      numpy tensor of RGB colours\n","    \"\"\"\n","    return colours[cats]\n","\n","def process(xs, ys, max_pixel=256.0, downsize_input=False):\n","    \"\"\"\n","    Pre-process CIFAR10 images by taking only the horse category,\n","    shuffling, and have colour values be bound between 0 and 1\n","\n","    Args:\n","      xs: the colour RGB pixel values\n","      ys: the category labels\n","      max_pixel: maximum pixel value in the original data\n","    Returns:\n","      xs: value normalized and shuffled colour images\n","      grey: greyscale images, also normalized so values are between 0 and 1\n","    \"\"\"\n","    xs = xs / max_pixel\n","    xs = xs[np.where(ys == HORSE_CATEGORY)[0], :, :, :]\n","    npr.shuffle(xs)\n","    \n","    grey = np.mean(xs, axis=1, keepdims=True)\n","\n","    if downsize_input:\n","      downsize_module = nn.Sequential(nn.AvgPool2d(2),\n","                               nn.AvgPool2d(2), \n","                               nn.Upsample(scale_factor=2), \n","                               nn.Upsample(scale_factor=2))\n","      xs_downsized = downsize_module.forward(torch.from_numpy(xs).float())\n","      xs_downsized = xs_downsized.data.numpy()\n","      return (xs, xs_downsized)\n","    else:\n","      return (xs, grey)\n","\n","\n","def get_batch(x, y, batch_size):\n","    '''\n","    Generated that yields batches of data\n","\n","    Args:\n","      x: input values\n","      y: output values\n","      batch_size: size of each batch\n","    Yields:\n","      batch_x: a batch of inputs of size at most batch_size\n","      batch_y: a batch of outputs of size at most batch_size\n","    '''\n","    N = np.shape(x)[0]\n","    assert N == np.shape(y)[0]\n","    for i in range(0, N, batch_size):\n","        batch_x = x[i:i+batch_size, :,:,:]\n","        batch_y = y[i:i+batch_size, :,:,:]\n","        yield (batch_x, batch_y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ft0qRJgWlK2q","colab_type":"text"},"source":["#### Torch helper"]},{"cell_type":"code","metadata":{"id":"TgMknlyhlJvi","colab_type":"code","colab":{}},"source":["def get_torch_vars(xs, ys, gpu=False):\n","    \"\"\"\n","    Helper function to convert numpy arrays to pytorch tensors.\n","    If GPU is used, move the tensors to GPU.\n","\n","    Args:\n","      xs (float numpy tenosor): greyscale input\n","      ys (int numpy tenosor): categorical labels \n","      gpu (bool): whether to move pytorch tensor to GPU\n","    Returns:\n","      Variable(xs), Variable(ys)\n","    \"\"\"\n","    xs = torch.from_numpy(xs).float()\n","    ys = torch.from_numpy(ys).long()\n","    if gpu:\n","        xs = xs.cuda()\n","        ys = ys.cuda()\n","    return Variable(xs), Variable(ys)\n","\n","def compute_loss(criterion, outputs, labels, batch_size, num_colours):\n","    \"\"\"\n","    Helper function to compute the loss. Since this is a pixelwise\n","    prediction task we need to reshape the output and ground truth\n","    tensors into a 2D tensor before passing it in to the loss criteron.\n","\n","    Args:\n","      criterion: pytorch loss criterion\n","      outputs (pytorch tensor): predicted labels from the model\n","      labels (pytorch tensor): ground truth labels\n","      batch_size (int): batch size used for training\n","      num_colours (int): number of colour categories\n","    Returns:\n","      pytorch tensor for loss\n","    \"\"\"\n","\n","    loss_out = outputs.transpose(1,3) \\\n","                      .contiguous() \\\n","                      .view([batch_size*32*32, num_colours])\n","    loss_lab = labels.transpose(1,3) \\\n","                      .contiguous() \\\n","                      .view([batch_size*32*32])\n","    return criterion(loss_out, loss_lab)\n","\n","def run_validation_step(cnn, criterion, test_grey, test_rgb_cat, batch_size,\n","                        colours, plotpath=None, visualize=True, downsize_input=False):\n","    correct = 0.0\n","    total = 0.0\n","    losses = []\n","    num_colours = np.shape(colours)[0]\n","    for i, (xs, ys) in enumerate(get_batch(test_grey,\n","                                           test_rgb_cat,\n","                                           batch_size)):\n","        images, labels = get_torch_vars(xs, ys, args.gpu)\n","        outputs = cnn(images)\n","\n","        val_loss = compute_loss(criterion,\n","                                outputs,\n","                                labels,\n","                                batch_size=args.batch_size,\n","                                num_colours=num_colours)\n","        losses.append(val_loss.data.item())\n","\n","        _, predicted = torch.max(outputs.data, 1, keepdim=True)\n","        total += labels.size(0) * 32 * 32\n","        correct += (predicted == labels.data).sum()\n","\n","    if plotpath: # only plot if a path is provided\n","        plot(xs, ys, predicted.cpu().numpy(), colours, \n","             plotpath, visualize=visualize, compare_bilinear=downsize_input)\n","\n","    val_loss = np.mean(losses)\n","    val_acc = 100 * correct / total\n","    return val_loss, val_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rp_wCpMjqt5w","colab_type":"text"},"source":["#### Visualization"]},{"cell_type":"code","metadata":{"id":"syg8NjwMqw_F","colab_type":"code","colab":{}},"source":["def plot(input, gtlabel, output, colours, path, visualize, compare_bilinear=False):\n","    \"\"\"\n","    Generate png plots of input, ground truth, and outputs\n","\n","    Args:\n","      input: the greyscale input to the colourization CNN\n","      gtlabel: the grouth truth categories for each pixel\n","      output: the predicted categories for each pixel\n","      colours: numpy array of colour categories and their RGB values\n","      path: output path\n","      visualize: display the figures inline or save the figures in path\n","    \"\"\"\n","    grey = np.transpose(input[:10,:,:,:], [0,2,3,1])\n","    gtcolor = get_cat_rgb(gtlabel[:10,0,:,:], colours)\n","    predcolor = get_cat_rgb(output[:10,0,:,:], colours)\n","\n","    img_stack = [\n","      np.hstack(np.tile(grey, [1,1,1,3])),\n","      np.hstack(gtcolor),\n","      np.hstack(predcolor)]\n","    \n","    if compare_bilinear:\n","      downsize_module = nn.Sequential(nn.AvgPool2d(2),\n","                                 nn.AvgPool2d(2), \n","                                 nn.Upsample(scale_factor=2, mode='bilinear'), \n","                                 nn.Upsample(scale_factor=2, mode='bilinear'))\n","      gt_input = np.transpose(gtcolor, [0, 3, 1, 2,])\n","      color_bilinear = downsize_module.forward(torch.from_numpy(gt_input).float())\n","      color_bilinear = np.transpose(color_bilinear.data.numpy(), [0, 2, 3, 1])\n","      img_stack = [\n","        np.hstack(np.transpose(input[:10,:,:,:], [0,2,3,1])),\n","        np.hstack(gtcolor),\n","        np.hstack(predcolor),\n","        np.hstack(color_bilinear)]\n","    img = np.vstack(img_stack)\n","    \n","    plt.grid('off')\n","    plt.imshow(img, vmin=0., vmax=1.)\n","    if visualize:\n","      plt.show()\n","    else:\n","      plt.savefig(path)\n","\n","def toimage(img, cmin, cmax):\n","    return Image.fromarray((img.clip(cmin, cmax)*255).astype(np.uint8))\n","  \n","def plot_activation(args, cnn):\n","    # LOAD THE COLOURS CATEGORIES\n","    colours = np.load(args.colours, allow_pickle=True)[0]\n","    num_colours = np.shape(colours)[0]\n","    \n","    (x_train, y_train), (x_test, y_test) = load_cifar10()\n","    test_rgb, test_grey = process(x_test, y_test, downsize_input = args.downsize_input)\n","    test_rgb_cat = get_rgb_cat(test_rgb, colours)\n","    \n","    # Take the idnex of the test image\n","    id = args.index\n","    outdir = \"outputs/\" + args.experiment_name + '/act' + str(id)\n","    if not os.path.exists(outdir):\n","      os.makedirs(outdir)\n","    images, labels = get_torch_vars(np.expand_dims(test_grey[id], 0),\n","                                    np.expand_dims(test_rgb_cat[id], 0))\n","    cnn.cpu()\n","    outputs = cnn(images)\n","    _, predicted = torch.max(outputs.data, 1, keepdim=True)\n","    predcolor = get_cat_rgb(predicted.cpu().numpy()[0,0,:,:], colours)\n","    img = predcolor\n","    toimage(predcolor, cmin=0, cmax=1) \\\n","            .save(os.path.join(outdir, \"output_%d.png\" % id))\n","\n","    if not args.downsize_input:\n","      img = np.tile(np.transpose(test_grey[id], [1,2,0]), [1,1,3])\n","    else:\n","      img = np.transpose(test_grey[id], [1,2,0])\n","    toimage(img, cmin=0, cmax=1) \\\n","            .save(os.path.join(outdir, \"input_%d.png\" % id))\n","\n","    img = np.transpose(test_rgb[id], [1,2,0])\n","    toimage(img, cmin=0, cmax=1) \\\n","            .save(os.path.join(outdir, \"input_%d_gt.png\" % id))\n","\n","    \n","    def add_border(img):\n","        return np.pad(img, 1, \"constant\", constant_values=1.0)\n","\n","    def draw_activations(path, activation, imgwidth=4):\n","        img = np.vstack([\n","            np.hstack([\n","                add_border(filter) for filter in\n","                activation[i*imgwidth:(i+1)*imgwidth,:,:]])\n","            for i in range(activation.shape[0] // imgwidth)])\n","        scipy.misc.imsave(path, img)\n","\n","\n","    for i, tensor in enumerate([cnn.out1, cnn.out2, cnn.out3, cnn.out4, cnn.out5]):\n","        draw_activations(\n","            os.path.join(outdir, \"conv%d_out_%d.png\" % (i+1, id)),\n","            tensor.data.cpu().numpy()[0])\n","    print(\"visualization results are saved to %s\"%outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIa_fiZYnRy7","colab_type":"text"},"source":["#### Training"]},{"cell_type":"code","metadata":{"id":"LtAdbbzHnP-n","colab_type":"code","colab":{}},"source":["class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","def train(args, cnn=None):\n","    # Set the maximum number of threads to prevent crash in Teaching Labs\n","    #TODO: necessary?\n","    torch.set_num_threads(5)\n","    # Numpy random seed\n","    npr.seed(args.seed)\n","    \n","    # Save directory\n","    save_dir = \"outputs/\" + args.experiment_name\n","\n","    # LOAD THE COLOURS CATEGORIES\n","    colours = np.load(args.colours, allow_pickle=True)[0]\n","    num_colours = np.shape(colours)[0]\n","    # INPUT CHANNEL\n","    num_in_channels = 1 if not args.downsize_input else 3\n","    # LOAD THE MODEL\n","    if cnn is None:\n","      if args.model == \"CNN\":\n","          cnn = CNN(args.kernel, args.num_filters, num_colours, num_in_channels)\n","      elif args.model == \"UNet\":\n","          cnn = UNet(args.kernel, args.num_filters, num_colours, num_in_channels)\n","\n","    # LOSS FUNCTION\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(cnn.parameters(), lr=args.learn_rate)\n","\n","    # DATA\n","    print(\"Loading data...\")\n","    (x_train, y_train), (x_test, y_test) = load_cifar10()\n","\n","    print(\"Transforming data...\")\n","    train_rgb, train_grey = process(x_train, y_train, downsize_input = args.downsize_input)\n","    train_rgb_cat = get_rgb_cat(train_rgb, colours)\n","    test_rgb, test_grey = process(x_test, y_test, downsize_input = args.downsize_input)\n","    test_rgb_cat = get_rgb_cat(test_rgb, colours)\n","\n","    # Create the outputs folder if not created already\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    print(\"Beginning training ...\")\n","    if args.gpu: cnn.cuda()\n","    start = time.time()\n","\n","    train_losses = []\n","    valid_losses = []\n","    valid_accs = []\n","    for epoch in range(args.epochs):\n","        # Train the Model\n","        cnn.train() # Change model to 'train' mode\n","        losses = []\n","        for i, (xs, ys) in enumerate(get_batch(train_grey,\n","                                               train_rgb_cat,\n","                                               args.batch_size)):\n","            images, labels = get_torch_vars(xs, ys, args.gpu)\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","            outputs = cnn(images)\n","\n","            loss = compute_loss(criterion,\n","                                outputs,\n","                                labels,\n","                                batch_size=args.batch_size,\n","                                num_colours=num_colours)\n","            loss.backward()\n","            optimizer.step()\n","            losses.append(loss.data.item())\n","\n","        # plot training images\n","        if args.plot:\n","            _, predicted = torch.max(outputs.data, 1, keepdim=True)\n","            plot(xs, ys, predicted.cpu().numpy(), colours,\n","                 save_dir+'/train_%d.png' % epoch, \n","                 args.visualize, \n","                 args.downsize_input)\n","\n","        # plot training images\n","        avg_loss = np.mean(losses)\n","        train_losses.append(avg_loss)\n","        time_elapsed = time.time() - start\n","        print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n","            epoch+1, args.epochs, avg_loss, time_elapsed))\n","\n","        # Evaluate the model\n","        cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n","        val_loss, val_acc = run_validation_step(cnn,\n","                                                criterion,\n","                                                test_grey,\n","                                                test_rgb_cat,\n","                                                args.batch_size,\n","                                                colours,\n","                                                save_dir+'/test_%d.png' % epoch,\n","                                                args.visualize,\n","                                                args.downsize_input)\n","\n","        time_elapsed = time.time() - start\n","        valid_losses.append(val_loss)\n","        valid_accs.append(val_acc)\n","        print('Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%, Time(s): %.2f' % (\n","            epoch+1, args.epochs, val_loss, val_acc, time_elapsed))\n","    \n","    # Plot training curve\n","    plt.figure()\n","    plt.plot(train_losses, \"ro-\", label=\"Train\")\n","    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","    plt.legend()\n","    plt.title(\"Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.savefig(save_dir+\"/training_curve.png\")\n","\n","    if args.checkpoint:\n","        print('Saving model...')\n","        torch.save(cnn.state_dict(), args.checkpoint)\n","    \n","    return cnn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAqGXV0iK1G9","colab_type":"text"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","## Question 1.\n","<!-- Complete the model CNN below. This model should have the same layers and convolutional filters as the RegressionCNN, with the exception of the output layer.  -->\n","\n","Complete the CNN model following the architecture described in the assignment handout."]},{"cell_type":"markdown","metadata":{"id":"AOTQISM45yoi","colab_type":"text"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"s8L5eIEF6BSa","colab_type":"text"},"source":["In the diagram, we denote the number of filters as **NF**. Further layers double the number of filters, denoted as **2NF**. In the final layers, the number of filters will be equivalent to the number of colour classes, denoted as **NC**. Consequently, your constructed neural network should define the number of input/output layers with respect to the variables `num_filters` and `num_colours`, as opposed to a constant value.\n","\n","\n","We will use our own convolution module MyConv2d instead of PyTorch's nn.Conv2D to better understand its internals.  \n","Each MyConv2d layer should be parameterized as follows:\n","- The number of channels in the image is defined in `num_in_channels`. Use this as the number of input filters for the first convolution.\n","- The number of output filters for each MyConv2d layer is specified after the hyphen. \n","- The kernel size used in MyConv2d should be specified via the `kernel` parameter.\n","\n","\n","For the remaining operations, you may use the default PyTorch implementations.  \n","The specific modules to use are listed below. If parameters are not otherwise specified, use the default PyTorch parameters.\n","\n","- [nn.MaxPool2d](https://pytorch.org/docs/stable/nn.html#maxpool2d): Use `kernel_size=2` for all layers.\n","\n","- [nn.BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d): The number of features is specified after the hyphen in the diagram as a multiple of **NF** or **NC**.\n","\n","- [nn.Upsample](https://pytorch.org/docs/stable/nn.html#upsample): Use `scaling_factor=2` for all layers.\n","\n","- [nn.ReLU](https://pytorch.org/docs/stable/nn.html#relu)\n","\n","\n","We recommend grouping each block of operations (those adjacent without whitespace in the diagram) into [nn.Sequential](https://pytorch.org/docs/stable/nn.html#sequential) containers.  \n","Grouping up relevant operations will allow for easier implementation of the `forward` method."]},{"cell_type":"code","metadata":{"id":"XOG_sFloK9gs","colab_type":"code","colab":{}},"source":["######################################################################\n","# MODELS\n","######################################################################\n","\n","class MyConv2d(nn.Module):\n","    \"\"\"\n","    Our simplified implemented of nn.Conv2d module for 2D convolution\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size, padding=None):\n","        super(MyConv2d, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        if padding is None:\n","            self.padding = kernel_size // 2\n","        else:\n","            self.padding = padding\n","        self.weight = nn.parameter.Parameter(torch.Tensor(\n","            out_channels, in_channels, kernel_size, kernel_size))\n","        self.bias = nn.parameter.Parameter(torch.Tensor(out_channels))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        n = self.in_channels * self.kernel_size * self.kernel_size\n","        stdv = 1. / math.sqrt(n)\n","        self.weight.data.uniform_(-stdv, stdv)\n","        self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input):\n","        return F.conv2d(input, self.weight, self.bias, padding=self.padding)\n","\n","\n","class CNN(nn.Module):\n","    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n","        super(CNN, self).__init__()\n","        padding = kernel // 2\n","\n","        ############### YOUR CODE GOES HERE ############### \n","        ###################################################\n","\n","    def forward(self, x):\n","        ############### YOUR CODE GOES HERE ############### \n","        ###################################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CTZWiuxMjQTB","colab_type":"text"},"source":["## Question 2.\n","Run main training loop of CNN in colourization.ipynb on Colab. This will train a CNN for a few epochs using the cross-entropy objective. It will generate some images showing the trained result at the end. Do these results look good to you? Why or why not?"]},{"cell_type":"code","metadata":{"id":"fZHc_eStGAQz","colab_type":"code","colab":{}},"source":["args = AttrDict()\n","args_dict = {\n","              'gpu':True, \n","              'valid':False, \n","              'checkpoint':\"\", \n","              'colours':'./data/colours/colour_kmeans24_cat7.npy', \n","              'model':\"CNN\", \n","              'kernel':3,\n","              'num_filters':32, \n","              'learn_rate':0.3, \n","              'batch_size':100, \n","              'epochs':25, \n","              'seed':0,\n","              'plot':True, \n","              'experiment_name': 'colourization_cnn',\n","              'visualize': False,\n","              'downsize_input':False,\n","}\n","args.update(args_dict)\n","cnn = train(args)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ULBGo6oNp2o","colab_type":"text"},"source":["## Question 3.\n","\n","Compute the number of weights, outputs, and connections in the model, as a function of **NF** and **NC**. Compute these values when each input dimension (width/height) is doubled. Report all 6 values.  "]},{"cell_type":"markdown","metadata":{"id":"GDUB0x1SNM16","colab_type":"text"},"source":["## Question 4.\n","\n"," Consider an pre-processing step where each input pixel is multiplied elementwise by scalar $a$, and is shifted by some scalar $b$. That is, where the original pixel value is denoted $x$, the new value is calculated $y = ax + b$. Assume this operation does not result in any overflows. How does this pre-processing step affect the output of the conv net from Question 1 and 2?"]},{"cell_type":"markdown","metadata":{"id":"Rb7Agthjf9qs","colab_type":"text"},"source":["# Part B. Skip Connections (2 pts)\n","\n","\n","A skip connection in a neural network is a connection which skips one or more layer and connects\n","to a later layer. We will introduce skip connections to our previous model."]},{"cell_type":"markdown","metadata":{"id":"C_DIdxbBgSjL","colab_type":"text"},"source":["## Question 1.\n","In this question, we will be adding a skip connection from the first layer to the last, second layer to the second last, etc. That is, the final convolution should have both the output of the previous layer and the initial greyscale input as input. This type of skip-connection is introduced by [Ronneberger et al.[2015]](https://arxiv.org/abs/1505.04597), and is called a ”UNet”. \n","\n","Just like the `CNN` class that you have completed in the previous part, complete the `__init__` and `forward` methods methods of the `UNet` class below.\n","\n","Hint: You will need to use the function `torch.cat`."]},{"cell_type":"code","metadata":{"id":"IgmrN71ImnOm","colab_type":"code","colab":{}},"source":["class UNet(nn.Module):\n","    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n","        super(UNet, self).__init__()\n","\n","        ############### YOUR CODE GOES HERE ############### \n","        ###################################################\n","\n","    def forward(self, x):\n","        ############### YOUR CODE GOES HERE ############### \n","        ###################################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KDCGvYMhgeFG","colab_type":"text"},"source":["## Question 2. \n","Train the ”UNet” model for at least 25 epochs and plot the training curve using a batch size of 100. "]},{"cell_type":"markdown","metadata":{"id":"ZXJzGP8inYF9","colab_type":"text"},"source":["#### Main training loop for UNet"]},{"cell_type":"code","metadata":{"id":"2-vGs7qHndmY","colab_type":"code","colab":{}},"source":["args = AttrDict()\n","args_dict = {\n","              'gpu':True, \n","              'valid':False, \n","              'checkpoint':\"\", \n","              'colours':'./data/colours/colour_kmeans24_cat7.npy', \n","              'model':\"UNet\", \n","              'kernel':3,\n","              'num_filters':32, \n","              'learn_rate':0.001, \n","              'batch_size':100, \n","              'epochs':25, \n","              'seed':0,\n","              'plot':True, \n","              'experiment_name': 'colourization_unet',\n","              'visualize': False,\n","              'downsize_input':False,\n","}\n","args.update(args_dict)\n","unet_cnn = train(args)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7S8D4rglqaj","colab_type":"text"},"source":["## Question 3.\n","How does the result compare to the previous model? Did skip connections improve the validation loss and accuracy? Did the skip connections improve the output qualitatively? How? Give at least two reasons why skip connections might improve the performance of our CNN models."]},{"cell_type":"markdown","metadata":{"id":"BsGziX_OhNBu","colab_type":"text"},"source":["## Question 4.\n","Re-train a few more ”UNet” models using different mini batch sizes with a fixed number of epochs. Describe the effect of batch sizes on the training/validation loss, and the final image output."]},{"cell_type":"markdown","metadata":{"id":"8t02DXBosDPQ","colab_type":"text"},"source":["# Part C. Finetune Semantic Segmentation Model (2 pts)\n","\n","In the previous two parts, we worked on training models for image colourization. Now we will switch gears and perform semantic segmentation by fine-tuning a pre-trained model. \n","        \n","*Semantic segmentation* can be considered as a pixel-wise classification problem where we need to predict the class label for each pixel. Fine-tuning is often used when you only have limited labeled data.\n","        \n","Here, we take a pre-trained model on the [Microsoft COCO dataset](https://arxiv.org/abs/1405.0312) and fine-tune it to perform segmentation with the classes it was never trained on. To be more specific, we use [**deeplabv3**](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/) pre-trained model and fine-tune it on the Oxford17 flower dataset.\n","        \n","We simplify the task to be a binary semantic segmentation task (background and flower). In the following code, you will first see some examples from the Oxford17 dataset and load the finetune the model by truncating the last layer of the network and replacing it with a randomly initialized convolutional layer. Note that we only update the weights of the newly introduced layer.\n"]},{"cell_type":"markdown","metadata":{"id":"svSjKTQAsLkB","colab_type":"text"},"source":["## Helper code\n","Below helper functions are provided for setting up the dataset and visualization."]},{"cell_type":"code","metadata":{"id":"XjBwlm5Ww68L","colab_type":"code","colab":{}},"source":["import cv2\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.io import loadmat\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lObva7r0w88D","colab_type":"text"},"source":["### Data related code"]},{"cell_type":"code","metadata":{"id":"Pw3yS9aJsKMK","colab_type":"code","colab":{}},"source":["# Dataset helper function\n","def read_image(path):\n","    im = cv2.imread(str(path))\n","    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","\n","def normalize(im):\n","    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n","    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n","    return (im/255.0 - imagenet_stats[0])/imagenet_stats[1]\n","\n","def denormalize(img):\n","    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n","    return img*imagenet_stats[1] + imagenet_stats[0]\n","\n","# Mainly imported from https://colab.research.google.com/drive/1KzGRSNQpP4BonRKj3ZwGMTGdi-e2y8z-?authuser=1#scrollTo=U_g_Rq1cA1Xi\n","class CUB(Dataset):\n","    def __init__(self, files_path, split, train=True):\n","      \n","        self.files_path = files_path\n","        self.split = split\n","        if train:\n","            filenames = list(self.split['trn1'][0]) + list(self.split['trn2'][0]) + list(self.split['trn3'][0])\n","        else:\n","            # We only use `val1` for validation\n","            filenames = self.split['val1'][0]\n","        \n","        valid_filenames = []\n","        for i in filenames:\n","            img_name = 'image_%04d.jpg' % int(i)\n","            if os.path.exists(os.path.join(files_path, 'jpg', img_name)) and \\\n","                os.path.exists(os.path.join(files_path, 'trimaps', img_name.replace('jpg', 'png'))):\n","                valid_filenames.append(img_name)\n","\n","        self.valid_filenames = valid_filenames\n","        self.num_files = len(valid_filenames)\n","       \n","    def __len__(self):\n","        return self.num_files\n","    \n","    def __getitem__(self, index):\n","        \n","        filename = self.valid_filenames[index]\n","\n","        # Load the image\n","        path = os.path.join(self.files_path, 'jpg', filename)\n","        x = read_image(path)  # H*W*c\n","        x = cv2.resize(x, (224,224))\n","        x = normalize(x)\n","        x = np.rollaxis(x, 2) # To meet torch's input specification(c*H*W) \n","\n","        # Load the segmentation mask\n","        path = os.path.join(self.files_path, 'trimaps', filename.replace(\"jpg\", \"png\"))\n","        y = read_image(path)\n","        y = cv2.resize(y, (224,224))  # H*W*c\n","        \n","        return x, y\n","\n","def initialize_loader(train_batch_size=64, val_batch_size=64):\n","    split = loadmat(\"datasplits.mat\")\n","    train_dataset = CUB('./', split, train= True)\n","    valid_dataset = CUB('./', split, train= False)\n","    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=val_batch_size, num_workers=4)\n","    return train_loader, valid_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFZXIMEwxUcj","colab_type":"text"},"source":["### Visualization"]},{"cell_type":"code","metadata":{"id":"FjQ3srxWxXWB","colab_type":"code","colab":{}},"source":["def visualize_dataset(dataloader):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    x, y = next(iter(dataloader))\n","    \n","    fig = plt.figure(figsize=(10, 5))\n","    for i in range(4):\n","      inp = x[i]\n","      inp = inp.numpy().transpose(1,2,0)\n","      inp = denormalize(inp)\n","      mask = y[i] / 255.\n","      \n","      ax = fig.add_subplot(2, 2, i+1, xticks=[], yticks=[])\n","      plt.imshow(np.concatenate([inp, mask], axis=1))\n","\n","def plot_prediction(args, model, is_train, index_list=[0], plotpath=None, title=None):\n","\n","    train_loader, valid_loader = initialize_loader()\n","    loader = train_loader if is_train else valid_loader\n","\n","    images, masks = next(iter(loader))\n","    images = images.float()\n","    if args.gpu:\n","        images = images.cuda()\n","\n","    with torch.no_grad():\n","        outputs = model(images)['out']\n","    output_predictions = outputs.argmax(1)\n","\n","    # create a color pallette, selecting a color for each class\n","    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n","    colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n","    colors = (colors % 255).numpy().astype(\"uint8\")\n","    colors = [i for color in colors for i in color]\n","\n","    for index in index_list:\n","        \n","        r = Image.fromarray(output_predictions[index].byte().cpu().numpy())\n","        r.putpalette(colors)\n","\n","        fig = plt.figure(figsize=(10, 5))\n","        if title: plt.title(title)\n","\n","        ax = fig.add_subplot(1, 3, 1, xticks=[], yticks=[])\n","        plt.imshow(denormalize(images[index].cpu().numpy().transpose(1,2,0)))\n","\n","        ax = fig.add_subplot(1, 3, 2, xticks=[], yticks=[])\n","        plt.imshow(r)\n","\n","        ax = fig.add_subplot(1, 3, 3, xticks=[], yticks=[])\n","        plt.imshow(masks[index])\n","\n","        if plotpath:\n","            plt.savefig(plotpath)\n","            plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNt3xOoqtDio","colab_type":"text"},"source":["## Download dataset and initialize DataLoader\n","Download the [Oxford17 Flower](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/) by running the code below. It will takes around 1 minutes for the first time.\n","\n"]},{"cell_type":"code","metadata":{"id":"cGdtgC1CtVyC","colab_type":"code","colab":{}},"source":["import os\n","if not os.path.exists(\"17flowers.tgz\"):\n","    print(\"Downloading flower dataset\")\n","    !wget https://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz\n","    !tar xvzf 17flowers.tgz\n","if not os.path.exists(\"trimaps.tgz\"):\n","    !wget https://www.robots.ox.ac.uk/~vgg/data/flowers/17/trimaps.tgz\n","    !tar xvzf trimaps.tgz\n","if not os.path.exists(\"datasplits.mat\"):\n","    !wget https://www.robots.ox.ac.uk/~vgg/data/flowers/17/datasplits.mat"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1YfiPPUtbjS","colab_type":"text"},"source":["Run the code below to initialize `DataLoader` and visualize few examples "]},{"cell_type":"code","metadata":{"id":"bPx8BywWts0q","colab_type":"code","colab":{}},"source":["train_loader, valid_loader = initialize_loader()\n","visualize_dataset(train_loader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8C0add3t7fp","colab_type":"text"},"source":["## Load pre-trained model\n","Pytorch [Hub](https://pytorch.org/docs/stable/hub.html) supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple hubconf.py file. Run the code below to download [deeplabv3](https://arxiv.org/pdf/1706.05587.pdf)."]},{"cell_type":"code","metadata":{"id":"Lnw1LJs9uO-C","colab_type":"code","colab":{}},"source":["# For further details, please refer to: https://arxiv.org/pdf/1706.05587.pds\n","model = torch.hub.load('pytorch/vision:v0.5.0', 'deeplabv3_resnet101', pretrained=True)\n","print(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zdq9El6rxnJy","colab_type":"text"},"source":["## Helper functions for training\n","Below are few functions helpful for model training."]},{"cell_type":"code","metadata":{"id":"LiM-8zefx4Kf","colab_type":"code","colab":{}},"source":["def compute_loss(pred, gt):\n","    loss = F.cross_entropy(pred, gt)\n","    return loss\n","\n","# from https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy\n","def iou_pytorch(outputs, labels):\n","    \n","    SMOOTH = 1e-6\n","    # You can comment out this line if you are passing tensors of equal shape\n","    # But if you are passing output from UNet or something it will most probably\n","    # be with the BATCH x 1 x H x W shape\n","    outputs = torch.argmax(outputs, 1)\n","    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n","    \n","    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n","    union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n","    \n","    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n","    \n","    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n","    \n","    return thresholded.mean()  # Or thresholded.mean() if you are interested in average across the batch\n","\n","def convert_to_binary(masks, thres=0.5):\n","    binary_masks = ((masks[:, 0, :, :] ==  128) & (masks[:, 1, :, :] == 0) & (masks[:, 2, :, :] == 0)) + 0.\n","    return binary_masks.long()\n","\n","def run_validation_step(args, epoch, model, loader, plotpath=None):\n","\n","    model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n","\n","    losses = []\n","    ious = []\n","    with torch.no_grad():\n","        for i, (images, masks) in enumerate(loader):\n","            permute_masks = masks.permute(0, 3, 1, 2)  # to match the input size: B, C, H, W\n","            binary_masks = convert_to_binary(permute_masks)\n","            if args.gpu:\n","                images = images.cuda()\n","                binary_masks = binary_masks.cuda()\n","            output = model(images.float())\n","            pred_seg_masks = output[\"out\"]\n","\n","            output_predictions = pred_seg_masks[0].argmax(0)\n","            loss = compute_loss(pred_seg_masks, binary_masks)\n","            iou = iou_pytorch(pred_seg_masks, binary_masks)\n","            losses.append(loss.data.item())\n","            ious.append(iou.data.item())\n","\n","        val_loss = np.mean(losses)\n","        val_iou = np.mean(ious)\n","    \n","    if plotpath:\n","        plot_prediction(args, model, False, index_list=[0], plotpath=plotpath, title='Val_%d' % epoch)\n","    \n","    return val_loss, val_iou"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uFZHlihYyCoL","colab_type":"text"},"source":["## Question 1.\n","For this assignment, we want to fine-tune only the last layer in our downloaded deeplabv3. We do this by keeping track of weights we want to update in `learned_parameters`. \n","            \n","Use the PyTorch utility [`Model.named_parameters()`](https://pytorch.org/docs/stable/nn.html), which returns an iterator over all the weight matrices of the model. \n","\n","The last layer weights have names prefix `classifier.4`. We will select the corresponding weights then passing them to `learned_parameters`.  \n","            \n","Complete the `train` function in Part C of the notebook by adding 2-3 lines of code where indicated.\n"]},{"cell_type":"code","metadata":{"id":"1ltM5w7SySxQ","colab_type":"code","colab":{}},"source":["def train(args, model):\n","    \n","    # Set the maximum number of threads to prevent crash in Teaching Labs\n","    torch.set_num_threads(5)\n","    # Numpy random seed\n","    np.random.seed(args.seed)\n","    \n","    # Save directory\n","    # Create the outputs folder if not created already\n","    save_dir = \"outputs/\" + args.experiment_name\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    learned_parameters = []\n","    # We only learn the last layer and freeze all the other weights \n","    ################ Code goes here ######################\n","    # Around 2-3 lines of code\n","    ######################################################\n","\n","    # Adam only updates learned_parameters\n","    optimizer = torch.optim.Adam(learned_parameters, lr=args.learn_rate)\n","\n","    train_loader, valid_loader = initialize_loader(args.train_batch_size, args.val_batch_size)\n","    print(\"Train set: {}, Test set: {}\".format(\n","            train_loader.dataset.num_files, valid_loader.dataset.num_files))\n","\n","    print(\"Beginning training ...\")\n","    if args.gpu: \n","        model.cuda()\n","\n","    start = time.time()\n","    trn_losses = []\n","    val_losses = []\n","    val_ious = []\n","    best_iou = None\n","\n","    for epoch in range(args.epochs):\n","\n","        # Train the Model\n","        model.train() # Change model to 'train' mode\n","        start_tr = time.time()\n","        \n","        losses = []\n","        for i, (images, masks) in enumerate(train_loader):\n","            permute_masks = masks.permute(0, 3, 1, 2)       # to match the input size: B, C, H, W\n","            binary_masks = convert_to_binary(permute_masks) # B, H, W\n","            if args.gpu:\n","                images = images.cuda()\n","                binary_masks = binary_masks.cuda()\n","\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","            output = model(images.float())\n","            pred_seg_masks = output[\"out\"]\n","\n","            _, pred_labels = torch.max(pred_seg_masks, 1, keepdim=True)\n","            loss = compute_loss(pred_seg_masks, binary_masks)\n","            loss.backward()\n","            optimizer.step()\n","            losses.append(loss.data.item())\n","\n","        # plot training images\n","        if args.plot:\n","            plot_prediction(args, model, True, index_list=[0], plotpath=save_dir+'/train_%d.png' % epoch, title='Train_%d' % epoch)\n","\n","        # plot training images\n","        trn_loss = np.mean(losses)\n","        trn_losses.append(trn_loss)\n","        time_elapsed = time.time() - start_tr\n","        print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n","                epoch+1, args.epochs, trn_loss, time_elapsed))\n","\n","        # Evaluate the model\n","        start_val = time.time()\n","        val_loss, val_iou = run_validation_step(args, \n","                                                epoch, \n","                                                model,\n","                                                valid_loader, \n","                                                save_dir+'/val_%d.png' % epoch)\n","\n","        if val_iou > best_iou:\n","            best_iou = val_iou\n","            torch.save(model.state_dict(), os.path.join(save_dir, args.checkpoint_name + '-best.ckpt'))\n","\n","        time_elapsed = time.time() - start_val\n","        print('Epoch [%d/%d], Loss: %.4f, mIOU: %.4f, Validation time (s): %d' % (\n","                epoch+1, args.epochs, val_loss, val_iou, time_elapsed))\n","        \n","        val_losses.append(val_loss)\n","        val_ious.append(val_iou)\n","\n","    # Plot training curve\n","    plt.figure()\n","    plt.plot(trn_losses, \"ro-\", label=\"Train\")\n","    plt.plot(val_losses, \"go-\", label=\"Validation\")\n","    plt.legend()\n","    plt.title(\"Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.savefig(save_dir+\"/training_curve.png\")\n","\n","    # Plot validation iou curve\n","    plt.figure()\n","    plt.plot(val_ious, \"ro-\", label=\"mIOU\")\n","    plt.legend()\n","    plt.title(\"mIOU\")\n","    plt.xlabel(\"Epochs\")\n","    plt.savefig(save_dir+\"/val_iou_curve.png\")\n","\n","    print('Saving model...')\n","    torch.save(model.state_dict(), os.path.join(save_dir, args.checkpoint_name + '-{}-last.ckpt'.format(args.epochs)))\n","\n","    print('Best model achieves mIOU: %.4f' % best_iou)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RB9DBRVqz5y-","colab_type":"text"},"source":["## Question 2.\n","For fine-tuning we also want to \n","* use `Model.requires_grad_()` to prevent back-prop through all the layers that should be frozen\n","* replace the last layer with a new `nn.Conv2d` with appropriate input output channels and kernel sizes. Since we are performing binary segmentation for this assignment, this new layer should have 2 output channels.\n","\n","Complete the script below by adding around 2 lines of code and train the model."]},{"cell_type":"code","metadata":{"id":"kTwzONiz0G3l","colab_type":"code","colab":{}},"source":["class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","args = AttrDict()\n","#You can play with the hyperparameters here, but to finish the assignment, \n","#there is no need to tune the hyperparameters here.\n","args_dict = {\n","              'gpu':True, \n","              'checkpoint_name':\"finetune-segmentation\", \n","              'learn_rate':0.05, \n","              'train_batch_size':128, \n","              'val_batch_size': 256, \n","              'epochs':10, \n","              'seed':0,\n","              'plot':True, \n","              'experiment_name': 'finetune-segmentation',\n","}\n","args.update(args_dict)\n","\n","#Truncate the last layer and replace it with the new one.\n","#To avoid `CUDA out of memory` error, you might find it useful (sometimes required) \n","#   to set the `requires_grad`=False for some layers\n","################ Code goes here ######################\n","# Around 2 lines of code\n","######################################################\n","\n","# Clear the cache in GPU\n","torch.cuda.empty_cache()\n","train(args, model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VB73XzxA0ZZ6","colab_type":"text"},"source":["## Question 3.\n","Visualize predictions by running the code below."]},{"cell_type":"code","metadata":{"id":"xYRjLxMP0fwj","colab_type":"code","colab":{}},"source":["plot_prediction(args, model, is_train=True, index_list=[0, 1, 2, 3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugtpTLKG0jq_","colab_type":"code","colab":{}},"source":["plot_prediction(args, model, is_train=False, index_list=[0, 1, 2, 3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rx-ch_iXajh1","colab_type":"text"},"source":["## Question 4.\n","\n","Consider a case of fine-tuning a pre-trained model with *n* number of layers. Each layers have the similar number of parameters, so the total number of parameters for the model is proportional to *n*. Describe the difference in memory complexity in terms of *n* between fine-tuning an entire pre-trained model versus fine-tuning only the last layer (freezing all the other layers). What about the computational complexity? "]},{"cell_type":"markdown","metadata":{"id":"KL5QpCxdazGx","colab_type":"text"},"source":["## Question 5.\n","\n","If we increase the height and the width of the input image by a factor of 2, how does this affect the memory complexity of fine-tuning? What about the number of parameters? "]}]}