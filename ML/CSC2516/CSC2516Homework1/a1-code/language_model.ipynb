{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr75-y9A4zcM"
   },
   "source": [
    "# Programming Assignment 1: Learning Distributed Word Representations\n",
    "Based on an assignment by George Dahl\n",
    "\n",
    "For CSC413/2516 in Winter 2020 with Professor Jimmy Ba\n",
    "\n",
    "**Submission:**\n",
    "You must submit two files through MarkUs: a PDF file containing your writeup, titled *a1-writeup.pdf*, and your code file *language_model.ipynb*. Your writeup must be typed.\n",
    "\n",
    "The programming assignments are individual work. See the Course Syllabus for detailed policies. \n",
    "\n",
    "**Introduction:**\n",
    "In this assignment we will learn about word embeddings and make neural networks learn about words.\n",
    "We could try to match statistics about the words, or we could train a network that takes a sequence of words as input and learns to predict the word that comes next.\n",
    "        \n",
    "This assignment will ask you to implement a linear embedding and then the backpropagation computations for a neural language model and then run some experiments to analyze the learned representation.\n",
    "The amount of code you have to write is very short but each line will require you to think very carefully.\n",
    "You will need to derive the updates mathematically, and then implement them using matrix and vector operations in NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UUSJPdr3Ge_"
   },
   "source": [
    "# Starter code and data\n",
    "\n",
    "Download and extract the archive from the course web page.\n",
    "Look at the file *raw_sentences.txt*.\n",
    "\n",
    "It contains the sentences that we will be using for this assignment.\n",
    "These sentences are fairly simple ones and cover a vocabulary of only 250 words.\n",
    "\n",
    "First, perform the required imports for your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRwuwhoJ3Knl"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pylab\n",
    "use_colab = False\n",
    "if use_colab:\n",
    "  from google.colab import drive\n",
    "\n",
    "TINY = 1e-30\n",
    "EPS = 1e-4\n",
    "nax = np.newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNLvRXdy3NDO"
   },
   "source": [
    "If you're using colaboratory, you can create a folder in your google drive - here we used 'CSC413_A1' - with the extracted contents of *a1-code.zip*.\n",
    "Then you can mount your drive to access the data.\n",
    "Feel free to use a different way to access the files *data.pk* and *partially_trained.pk*.\n",
    "You may have to use to enter an authorization code to mount your drive if you use the google drive to store the data.\n",
    "\n",
    "If you're not using colaboratory, then set the paths to wherever the extracted contents are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gkug8am63SzY"
   },
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "  drive_name = '/content/drive'\n",
    "  drive.mount(drive_name)\n",
    "  drive_413_A1_folder = 'CSC413_A1'\n",
    "  drive_location = drive_name + '/My Drive/' + drive_413_A1_folder  # Change this to where your files are located\n",
    "else:\n",
    "  # set the drive_location variable to whereever the extracted contents are.\n",
    "  drive_location = '.'\n",
    "\n",
    "data_location = drive_location + '/' + 'data.pk'\n",
    "PARTIALLY_TRAINED_MODEL = drive_location + '/' + 'partially_trained.pk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qna9z_wJ3U5e"
   },
   "source": [
    "We have already extracted the 4-grams from this dataset and divided them into training, validation, and test sets.\n",
    "To inspect this data, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD1LN16d3a0u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'set', 'just', 'show', 'being', 'money', 'over', 'both', 'years', 'four']\n",
      "[[ 27  25  89]\n",
      " [183  43 248]\n",
      " [182  31  75]\n",
      " [116 246 200]\n",
      " [222 189 248]\n",
      " [ 41  73  25]\n",
      " [241  31 222]\n",
      " [222  31 157]\n",
      " [ 73  31 220]\n",
      " [ 41 191  90]]\n",
      "[143 116 121 185   5  31  31 143  31  67]\n",
      "(372500, 3) (372500,)\n",
      "(46500, 3) (46500,)\n",
      "(46500, 3) (46500,)\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(data_location, 'rb'))\n",
    "print(data['vocab'][:10])\n",
    "print(data['train_inputs'][:10])\n",
    "print(data['train_targets'][:10])\n",
    "print(data['train_inputs'].shape, data['train_targets'].shape)\n",
    "print(data['valid_inputs'].shape, data['valid_targets'].shape)\n",
    "print(data['test_inputs'].shape, data['test_targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXd2Msqs3fPQ"
   },
   "source": [
    "Now *data* is a Python dict which contains the vocabulary, as well as the inputs and targets for all three splits of the data. *data*['vocab'] is a list of the 250 words in the dictionary; *data*['vocab'][0] is the word with index 0, and so on. *data*['train_inputs'] is a 372,500 x 3 matrix where each row gives the indices of the 3 context words for one of the 372,500 training cases.\n",
    "*data*['train_targets'] is a vector giving the index of the target word for each training case. The validation and test sets are handled analogously.\n",
    "\n",
    "Even though you only have to modify two specific locations in the code, you may want to read through this code before starting the assignment. \n",
    "\n",
    "    {\n",
    "        'vocab': ['all', 'set', 'just', ...]\n",
    "        'train_inputs': [[27 25 89], [...], ...]\n",
    "        'train_targets': [143 116 121 ...]\n",
    "        'valid_inputs': ...\n",
    "        'valid_targets': ...\n",
    "        'test_inputs': ...\n",
    "        'test_targets': ...\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa9ggqxJPPs0"
   },
   "source": [
    "# Part 1: GLoVE Word Representations (4pts)\n",
    "\n",
    "In this part of the assignment, you will implement a simplified version of the GLoVE embedding (please see the handout for detailed description of the algorithm) with the loss defined as\n",
    "\n",
    "$L(\\{\\mathbf{w}_i,b_i\\}_{i=1}^V) = \\sum_{i,j=1}^V (\\mathbf{w}_i^\\top\\mathbf{w}_j + b_i + b_j - \\log X_{ij})^2$.\n",
    "\n",
    "Note that each word is represented by a d-dimensional vector $\\mathbf{w}_i$ and a scalar bias $b_i$.\n",
    "\n",
    "We have provided a few functions for training the embedding:\n",
    "\n",
    "*   *calculate_log_co_occurence* computes the log co-occurrence matrix of a given corpus\n",
    "*   *train_GLoVE* runs momentum gradient descent to optimize the embedding\n",
    "*   *loss_GLoVE:* INPUT - $V\\times d$ matrix $W$ (collection of $V$ embedding vectors, each d-dimensional); $V\\times 1$ vector $\\mathbf{b}$ (collection of $V$ bias terms); $V \\times V$ log co-occurrence matrix. OUTPUT - loss of the GLoVE objective\n",
    "*   *grad_GLoVE:* INPUT - $V\\times d$ matrix $W$, $V\\times 1$ vector b, and $V\\times V$ log co-occurrence matrix. OUTPUT - $V\\times d$ matrix grad_W containing the gradient of the loss function w.r.t. $W$; $V\\times 1$ vector grad_b which is the gradient of the loss function w.r.t. $\\mathbf{b}$. TO BE IMPLEMENTED.\n",
    "\n",
    "Run the code to compute the co-occurence matrix.\n",
    "Make sure to add a 1 to the occurences, so there are no 0's in the matrix when we take the elementwise log of the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbYUnHGQNbgu"
   },
   "outputs": [],
   "source": [
    "vocab_size = 250\n",
    "\n",
    "def calculate_log_co_occurence(word_data):\n",
    "  \"Compute the log-co-occurence matrix for our data.\"\n",
    "  log_co_occurence = np.zeros((vocab_size, vocab_size))\n",
    "  for input in word_data:\n",
    "    log_co_occurence[input[0], input[1]] += 1\n",
    "    log_co_occurence[input[1], input[2]] += 1\n",
    "    # If we want symmetric co-occurence can also increment for these.\n",
    "    # Optional: How would you generalize the model if our target co-occurence isn't symmetric?\n",
    "    log_co_occurence[input[1], input[0]] += 1\n",
    "    log_co_occurence[input[2], input[1]] += 1\n",
    "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
    "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
    "  log_co_occurence = np.log(log_co_occurence)\n",
    "  return log_co_occurence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsIN6RrwONPf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250) (250, 250)\n"
     ]
    }
   ],
   "source": [
    "log_co_occurence_train = calculate_log_co_occurence(data['train_inputs'])\n",
    "log_co_occurence_valid = calculate_log_co_occurence(data['valid_inputs'])\n",
    "print(log_co_occurence_train.shape, log_co_occurence_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNnKkMy-d2bB"
   },
   "source": [
    "TO BE IMPLEMENTED: calculate the gradient of the loss function w.r.t. the parameters $W$ and $\\mathbf{b}$. You should vectorize the computation, i.e. not loop over every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbpkXeaAdwnj"
   },
   "outputs": [],
   "source": [
    "def loss_GLoVE(W, b, log_co_occurence):\n",
    "  \"Compute the GLoVE loss.\"\n",
    "  n,_ = log_co_occurence.shape\n",
    "  return np.sum((W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence)**2)\n",
    "\n",
    "def grad_GLoVE(W,  b, log_co_occurence):\n",
    "  \"Return the gradient of GLoVE objective w.r.t W and b.\"\n",
    "  \"INPUT: W - Vxd; b - Vx1; log_co_occurence: VxV\"\n",
    "  \"OUTPUT: grad_W - Vxd; grad_b - Vx1\"\n",
    "  n,_ = log_co_occurence.shape\n",
    "  ###########################   YOUR CODE HERE  ##############################\n",
    "  WW = W.dot(W.T) + b.dot(np.ones([1,n])) + np.ones([n,1]).dot(b.T) - log_co_occurence\n",
    "  grad_W = WW.dot(W) * 4\n",
    "  grad_b = WW.dot(np.ones([n, 1])) * 4\n",
    "  ############################################################################\n",
    "  return grad_W, grad_b\n",
    "\n",
    "def train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
    "  \"Traing W and b according to GLoVE objective.\"\n",
    "  n,_ = log_co_occurence_train.shape\n",
    "  learning_rate = 0.1 / n  # A hyperparameter.  You can play with this if you want.\n",
    "  for epoch in range(n_epochs):\n",
    "    grad_W, grad_b = grad_GLoVE(W, b, log_co_occurence_train)\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "    train_loss, valid_loss = loss_GLoVE(W, b, log_co_occurence_train), loss_GLoVE(W, b, log_co_occurence_valid)\n",
    "    if do_print:\n",
    "      print(f\"Train Loss: {train_loss}, valid loss: {valid_loss}, grad_norm: {np.sum(grad_W**2)}\")\n",
    "  return W, b, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjiNQ0WkWi1Z"
   },
   "source": [
    "Train the GLoVE model for a range of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46yGUezEMLJe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:17<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n_epochs = 500  # A hyperparameter.  You can play with this if you want.\n",
    "embedding_dims = np.array([1,2,3,5,7,10,12,15,20,25,30,40,50])  # Play with this\n",
    "final_train_losses, final_val_losses = [], []  # Store the final losses for graphing\n",
    "W_final_2d, b_final_2d = None, None\n",
    "do_print = False  # If you want to see diagnostic information during training\n",
    "for embedding_dim in tqdm(embedding_dims):\n",
    "  init_variance = 0.1  # A hyperparameter.  You can play with this if you want.\n",
    "  W = init_variance * np.random.normal(size=(250, embedding_dim))\n",
    "  b = init_variance * np.random.normal(size=(250, 1))\n",
    "  if do_print:\n",
    "    print(f\"Training for embedding dimension: {embedding_dim}\")\n",
    "  W_final, b_final, train_loss, valid_loss = train_GLoVE(W, b, log_co_occurence_train, \n",
    "                                                         log_co_occurence_valid, n_epochs, \n",
    "                                                         do_print=do_print)\n",
    "  if embedding_dim == 2:\n",
    "    # Save a parameter copy if we are training 2d embedding for visualization later\n",
    "    W_final_2d = W_final\n",
    "    b_final_2d = b_final\n",
    "  final_train_losses += [train_loss]\n",
    "  final_val_losses += [valid_loss]\n",
    "  if do_print:\n",
    "    print(f\"Final validation loss: {valid_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzV-qFf5WfAp"
   },
   "source": [
    "Plot the training and validation losses against the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHgHgSzJTg5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f71f7fb4128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.loglog(embedding_dims, final_train_losses)\n",
    "pylab.xlabel(\"Embedding Dimension\")\n",
    "pylab.ylabel(\"Training Loss\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UJSfg_hfvIV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 107541.08362454457)\n",
      "(2, 106279.43177423264)\n",
      "(3, 106366.00538872746)\n",
      "(5, 106238.73608646847)\n",
      "(7, 105941.73045164737)\n",
      "(10, 105746.84836694665)\n",
      "(12, 105760.25459596483)\n",
      "(15, 106239.14938680865)\n",
      "(20, 106903.66402709665)\n",
      "(25, 107561.90332703466)\n",
      "(30, 108204.6817291133)\n",
      "(40, 109089.24279593707)\n",
      "(50, 110019.16625795835)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f71f7ce6630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in zip(embedding_dims, final_val_losses):\n",
    "  print(i)\n",
    "pylab.loglog(embedding_dims, final_val_losses)\n",
    "pylab.xlabel(\"Embedding Dimension\")\n",
    "pylab.ylabel(\"Validation Loss\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xo1R6rfP4aJQ"
   },
   "source": [
    "Answer the following questions:\n",
    "\n",
    "1.  Given the vocabulary size $V$ and embedding dimensionality $d$, how many parameters does the GLoVE model have?\n",
    "2.  Write the gradient of the loss function with respect to one parameter vector $\\mathbf{w}_i$.\n",
    "3.  Implement the gradient update of GLoVE.\n",
    "4.  Train the model with varying dimensionality $d$.\n",
    "Which $d$ leads to optimal validation performance?\n",
    "Why does / doesn't larger $d$ always lead to better validation error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YwDZcOhjywe"
   },
   "source": [
    "# Part 2: Network Architecture (2pts)\n",
    "See the handout for the written questions in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8FZTyrzlCNl"
   },
   "source": [
    "# Part 3: Training the model (4pts)\n",
    "\n",
    "There are three classes defined in this *part*: *Params*, *Activations*, *Model*.\n",
    "You will make changes to *Model*, but it may help to read through *Params* and *Activations* first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfGEjB3QLNXf"
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    \"\"\"A class representing the trainable parameters of the model. This class has five fields:\n",
    "    \n",
    "           word_embedding_weights, a matrix of size N_V x D, where N_V is the number of words in the vocabulary\n",
    "                   and D is the embedding dimension.\n",
    "           embed_to_hid_weights, a matrix of size N_H x 3D, where N_H is the number of hidden units. The first D\n",
    "                   columns represent connections from the embedding of the first context word, the next D columns\n",
    "                   for the second context word, and so on.\n",
    "           hid_bias, a vector of length N_H\n",
    "           hid_to_output_weights, a matrix of size N_V x N_H\n",
    "           output_bias, a vector of length N_V\"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
    "                 hid_bias, output_bias):\n",
    "        self.word_embedding_weights = word_embedding_weights\n",
    "        self.embed_to_hid_weights = embed_to_hid_weights\n",
    "        self.hid_to_output_weights = hid_to_output_weights\n",
    "        self.hid_bias = hid_bias\n",
    "        self.output_bias = output_bias\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(self.word_embedding_weights.copy(), self.embed_to_hid_weights.copy(),\n",
    "                              self.hid_to_output_weights.copy(), self.hid_bias.copy(), self.output_bias.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, vocab_size, context_len, embedding_dim, num_hid):\n",
    "        \"\"\"A constructor which initializes all weights and biases to 0.\"\"\"\n",
    "        word_embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "        embed_to_hid_weights = np.zeros((num_hid, context_len * embedding_dim))\n",
    "        hid_to_output_weights = np.zeros((vocab_size, num_hid))\n",
    "        hid_bias = np.zeros(num_hid)\n",
    "        output_bias = np.zeros(vocab_size)\n",
    "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
    "                   hid_bias, output_bias)\n",
    "\n",
    "    @classmethod\n",
    "    def random_init(cls, init_wt, vocab_size, context_len, embedding_dim, num_hid):\n",
    "        \"\"\"A constructor which initializes weights to small random values and biases to 0.\"\"\"\n",
    "        word_embedding_weights = np.random.normal(0., init_wt, size=(vocab_size, embedding_dim))\n",
    "        embed_to_hid_weights = np.random.normal(0., init_wt, size=(num_hid, context_len * embedding_dim))\n",
    "        hid_to_output_weights = np.random.normal(0., init_wt, size=(vocab_size, num_hid))\n",
    "        hid_bias = np.zeros(num_hid)\n",
    "        output_bias = np.zeros(vocab_size)\n",
    "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
    "                   hid_bias, output_bias)\n",
    "\n",
    "    ###### The functions below are Python's somewhat oddball way of overloading operators, so that\n",
    "    ###### we can do arithmetic on Params instances. You don't need to understand this to do the assignment.\n",
    "\n",
    "    def __mul__(self, a):\n",
    "        return self.__class__(a * self.word_embedding_weights,\n",
    "                              a * self.embed_to_hid_weights,\n",
    "                              a * self.hid_to_output_weights,\n",
    "                              a * self.hid_bias,\n",
    "                              a * self.output_bias)\n",
    "\n",
    "    def __rmul__(self, a):\n",
    "        return self * a\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.__class__(self.word_embedding_weights + other.word_embedding_weights,\n",
    "                              self.embed_to_hid_weights + other.embed_to_hid_weights,\n",
    "                              self.hid_to_output_weights + other.hid_to_output_weights,\n",
    "                              self.hid_bias + other.hid_bias,\n",
    "                              self.output_bias + other.output_bias)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + -1. * other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6XFQUPsLSi7"
   },
   "outputs": [],
   "source": [
    "class Activations(object):\n",
    "    \"\"\"A class representing the activations of the units in the network. This class has three fields:\n",
    "\n",
    "        embedding_layer, a matrix of B x 3D matrix (where B is the batch size and D is the embedding dimension),\n",
    "                representing the activations for the embedding layer on all the cases in a batch. The first D\n",
    "                columns represent the embeddings for the first context word, and so on.\n",
    "        hidden_layer, a B x N_H matrix representing the hidden layer activations for a batch\n",
    "        output_layer, a B x N_V matrix representing the output layer activations for a batch\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_layer, hidden_layer, output_layer):\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "def get_batches(inputs, targets, batch_size, shuffle=True):\n",
    "    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n",
    "    'generator', i.e. something you can use in a for loop. You don't need to understand how it\n",
    "    works to do the assignment.\"\"\"\n",
    "\n",
    "    if inputs.shape[0] % batch_size != 0:\n",
    "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
    "    num_batches = inputs.shape[0] // batch_size\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "        inputs = inputs[idxs, :]\n",
    "        targets = targets[idxs]\n",
    "\n",
    "    for m in range(num_batches):\n",
    "        yield inputs[m * batch_size:(m + 1) * batch_size, :], \\\n",
    "              targets[m * batch_size:(m + 1) * batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuAXaDNll0lf"
   },
   "source": [
    "In this part of the assignment, you implement a method which computes the gradient using backpropagation.\n",
    "To start you out, the *Model* class contains several important methods used in training:\n",
    "\n",
    "\n",
    "*   *compute_activations* computes the activations of all units on a given input batch\n",
    "*   *compute_loss* computes the total cross-entropy loss on a mini-batch\n",
    "*   *evaluate* computes the average cross-entropy loss for a given set of inputs and targets\n",
    "\n",
    "You will need to complete the implementation of two additional methods which are needed for training:\n",
    "\n",
    "\n",
    "*   *compute_loss_derivative* computes the derivative of the loss function with respect to the output layer inputs.\n",
    "*   *back_propagate* is the function which computes the gradient of the loss with respect to model parameters using backpropagation.\n",
    "It uses the derivatives computed by *compute_loss_derivative*.\n",
    "Some parts are already filled in for you, but you need to compute the matrices of derivatives for *embed_to_hid_weights*, *hid_bias*, *hid_to_output_weights*, and *output_bias*.\n",
    "These matrices have the same sizes as the parameter matrices (see previous section).\n",
    "\n",
    "In order to implement backpropagation efficiently, you need to express the computations in terms of matrix operations, rather than *for* loops.\n",
    "You should first work through the derivatives on pencil and paper.\n",
    "First, apply the chain rule to compute the derivatives with respect to individual units, weights, and biases.\n",
    "Next, take the formulas you've derived, and express them in matrix form.\n",
    "You should be able to express all of the required computations using only matrix multiplication, matrix transpose, and elementwise operations --- no *for* loops!\n",
    "If you want inspiration, read through the code for *Model.compute_activations* and try to understand how the matrix operations correspond to the computations performed by all the units in the network.\n",
    "        \n",
    "To make your life easier, we have provided the routine *checking.check_gradients*, which checks your gradients using finite differences.\n",
    "You should make sure this check passes before continuing with the assignment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0F4CTBipK9B6"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "    A class representing the language model itself. This class contains various methods used in training\n",
    "    the model and visualizing the learned representations. It has two fields:\n",
    "\n",
    "    params, a Params instance which contains the model parameters\n",
    "    vocab, a list containing all the words in the dictionary; vocab[0] is the word with index 0, and so on.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, vocab):\n",
    "        self.params = params\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding_dim = self.params.word_embedding_weights.shape[1]\n",
    "        self.embedding_layer_dim = self.params.embed_to_hid_weights.shape[1]\n",
    "        self.context_len = self.embedding_layer_dim // self.embedding_dim\n",
    "        self.num_hid = self.params.embed_to_hid_weights.shape[0]\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(self.params.copy(), self.vocab[:])\n",
    "\n",
    "    @classmethod\n",
    "    def random_init(cls, init_wt, vocab, context_len, embedding_dim, num_hid):\n",
    "        \"\"\"Constructor which randomly initializes the weights to Gaussians with standard deviation init_wt\n",
    "        and initializes the biases to all zeros.\"\"\"\n",
    "        params = Params.random_init(init_wt, len(vocab), context_len, embedding_dim, num_hid)\n",
    "        return Model(params, vocab)\n",
    "\n",
    "    def indicator_matrix(self, targets):\n",
    "        \"\"\"Construct a matrix where the kth entry of row i is 1 if the target for example\n",
    "        i is k, and all other entries are 0.\"\"\"\n",
    "        batch_size = targets.size\n",
    "        expanded_targets = np.zeros((batch_size, len(self.vocab)))\n",
    "        expanded_targets[np.arange(batch_size), targets] = 1.\n",
    "        return expanded_targets\n",
    "\n",
    "    def compute_loss_derivative(self, output_activations, expanded_target_batch):\n",
    "        \"\"\"Compute the derivative of the cross-entropy loss function with respect to the inputs\n",
    "        to the output units. In particular, the output layer computes the softmax\n",
    "\n",
    "            y_i = e^{z_i} / \\sum_j e^{z_j}.\n",
    "\n",
    "        This function should return a batch_size x vocab_size matrix, where the (i, j) entry\n",
    "        is dC / dz_j computed for the ith training case, where C is the loss function\n",
    "\n",
    "            C = -sum(t_i log y_i).\n",
    "\n",
    "        The arguments are as follows:\n",
    "\n",
    "            output_activations - the activations of the output layer, i.e. the y_i's.\n",
    "            expanded_target_batch - each row is the indicator vector for a target word,\n",
    "                i.e. the (i, j) entry is 1 if the i'th word is j, and 0 otherwise.\"\"\"\n",
    "\n",
    "        ###########################   YOUR CODE HERE  ##############################\n",
    "        return output_activations - expanded_target_batch\n",
    "        ############################################################################\n",
    "\n",
    "    def compute_loss(self, output_activations, expanded_target_batch):\n",
    "        \"\"\"Compute the total loss over a mini-batch. expanded_target_batch is the matrix obtained\n",
    "        by calling indicator_matrix on the targets for the batch.\"\"\"\n",
    "        return -np.sum(expanded_target_batch * np.log(output_activations + TINY))\n",
    "\n",
    "    def compute_activations(self, inputs):\n",
    "        \"\"\"Compute the activations on a batch given the inputs. Returns an Activations instance.\n",
    "        You should try to read and understand this function, since this will give you clues for\n",
    "        how to implement back_propagate.\"\"\"\n",
    "\n",
    "        batch_size = inputs.shape[0]\n",
    "        if inputs.shape[1] != self.context_len:\n",
    "            raise RuntimeError('Dimension of the input vectors should be {}, but is instead {}'.format(\n",
    "                self.context_len, inputs.shape[1]))\n",
    "\n",
    "        # Embedding layer\n",
    "        # Look up the input word indies in the word_embedding_weights matrix\n",
    "        embedding_layer_state = np.zeros((batch_size, self.embedding_layer_dim))\n",
    "        for i in range(self.context_len):\n",
    "            embedding_layer_state[:, i * self.embedding_dim:(i + 1) * self.embedding_dim] = \\\n",
    "                self.params.word_embedding_weights[inputs[:, i], :]\n",
    "\n",
    "        # Hidden layer\n",
    "        inputs_to_hid = np.dot(embedding_layer_state, self.params.embed_to_hid_weights.T) + \\\n",
    "                        self.params.hid_bias\n",
    "        # Apply logistic activation function\n",
    "        hidden_layer_state = 1. / (1. + np.exp(-inputs_to_hid))\n",
    "\n",
    "        # Output layer\n",
    "        inputs_to_softmax = np.dot(hidden_layer_state, self.params.hid_to_output_weights.T) + \\\n",
    "                            self.params.output_bias\n",
    "\n",
    "        # Subtract maximum.\n",
    "        # Remember that adding or subtracting the same constant from each input to a\n",
    "        # softmax unit does not affect the outputs. So subtract the maximum to\n",
    "        # make all inputs <= 0. This prevents overflows when computing their exponents.\n",
    "        inputs_to_softmax -= inputs_to_softmax.max(1).reshape((-1, 1))\n",
    "\n",
    "        output_layer_state = np.exp(inputs_to_softmax)\n",
    "        output_layer_state /= output_layer_state.sum(1).reshape((-1, 1))\n",
    "\n",
    "        return Activations(embedding_layer_state, hidden_layer_state, output_layer_state)\n",
    "\n",
    "    def back_propagate(self, input_batch, activations, loss_derivative):\n",
    "        \"\"\"Compute the gradient of the loss function with respect to the trainable parameters\n",
    "        of the model. The arguments are as follows:\n",
    "\n",
    "             input_batch - the indices of the context words\n",
    "             activations - an Activations class representing the output of Model.compute_activations\n",
    "             loss_derivative - the matrix of derivatives computed by compute_loss_derivative\n",
    "             \n",
    "        Part of this function is already completed, but you need to fill in the derivative\n",
    "        computations for hid_to_output_weights_grad, output_bias_grad, embed_to_hid_weights_grad,\n",
    "        and hid_bias_grad. See the documentation for the Params class for a description of what\n",
    "        these matrices represent.\"\"\"\n",
    "\n",
    "        # The matrix with values dC / dz_j, where dz_j is the input to the jth hidden unit,\n",
    "        # i.e. y_j = 1 / (1 + e^{-z_j})\n",
    "        hid_deriv = np.dot(loss_derivative, self.params.hid_to_output_weights) \\\n",
    "                    * activations.hidden_layer * (1. - activations.hidden_layer)\n",
    "\n",
    "        ###########################   YOUR CODE HERE  ##############################\n",
    "        output_bias_grad = np.sum(loss_derivative, 0)  # V\n",
    "        hid_to_output_weights_grad = loss_derivative.T.dot(activations.hidden_layer)  # 128 x V\n",
    "        # hid_grad = loss_derivative.dot(self.params.hid_to_output_weights)  # B x 128\n",
    "        hid_bias_grad = np.sum(hid_deriv, axis=0)  # 128\n",
    "        embed_to_hid_weights_grad = hid_deriv.T.dot(activations.embedding_layer)  # 48 x 128\n",
    "        ############################################################################\n",
    "\n",
    "        # The matrix of derivatives for the embedding layer\n",
    "        embed_deriv = np.dot(hid_deriv, self.params.embed_to_hid_weights)\n",
    "\n",
    "        # Embedding layer\n",
    "        word_embedding_weights_grad = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        for w in range(self.context_len):\n",
    "            word_embedding_weights_grad += np.dot(self.indicator_matrix(input_batch[:, w]).T,\n",
    "                                                  embed_deriv[:, w * self.embedding_dim:(w + 1) * self.embedding_dim])\n",
    "\n",
    "        return Params(word_embedding_weights_grad, embed_to_hid_weights_grad, hid_to_output_weights_grad,\n",
    "                      hid_bias_grad, output_bias_grad)\n",
    "\n",
    "    def evaluate(self, inputs, targets, batch_size=100):\n",
    "        \"\"\"Compute the average cross-entropy over a dataset.\n",
    "\n",
    "            inputs: matrix of shape D x N\n",
    "            targets: one-dimensional matrix of length N\"\"\"\n",
    "\n",
    "        ndata = inputs.shape[0]\n",
    "\n",
    "        total = 0.\n",
    "        for input_batch, target_batch in get_batches(inputs, targets, batch_size):\n",
    "            activations = self.compute_activations(input_batch)\n",
    "            expanded_target_batch = self.indicator_matrix(target_batch)\n",
    "            cross_entropy = -np.sum(expanded_target_batch * np.log(activations.output_layer + TINY))\n",
    "            total += cross_entropy\n",
    "\n",
    "        return total / float(ndata)\n",
    "\n",
    "    def display_nearest_words(self, word, k=10):\n",
    "        \"\"\"List the k words nearest to a given word, along with their distances.\"\"\"\n",
    "\n",
    "        if word not in self.vocab:\n",
    "            print('Word \"{}\" not in vocabulary.'.format(word))\n",
    "            return\n",
    "\n",
    "        # Compute distance to every other word.\n",
    "        idx = self.vocab.index(word)\n",
    "        word_rep = self.params.word_embedding_weights[idx, :]\n",
    "        diff = self.params.word_embedding_weights - word_rep.reshape((1, -1))\n",
    "        distance = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "\n",
    "        # Sort by distance.\n",
    "        order = np.argsort(distance)\n",
    "        order = order[1:1 + k]  # The nearest word is the query word itself, skip that.\n",
    "        for i in order:\n",
    "            print('{}: {}'.format(self.vocab[i], distance[i]))\n",
    "\n",
    "    def predict_next_word(self, word1, word2, word3, k=10):\n",
    "        \"\"\"List the top k predictions for the next word along with their probabilities.\n",
    "        Inputs:\n",
    "            word1: The first word as a string.\n",
    "            word2: The second word as a string.\n",
    "            word3: The third word as a string.\n",
    "            k: The k most probable predictions are shown.\n",
    "        Example usage:\n",
    "            model.predict_next_word('john', 'might', 'be', 3)\n",
    "            model.predict_next_word('life', 'in', 'new', 3)\"\"\"\n",
    "\n",
    "        if word1 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "        if word2 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "        if word3 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n",
    "\n",
    "        idx1, idx2, idx3 = self.vocab.index(word1), self.vocab.index(word2), self.vocab.index(word3)\n",
    "        input = np.array([idx1, idx2, idx3]).reshape((1, -1))\n",
    "        activations = self.compute_activations(input)\n",
    "        prob = activations.output_layer.ravel()\n",
    "        idxs = np.argsort(prob)[::-1]  # sort descending\n",
    "        for i in idxs[:k]:\n",
    "            print('{} {} {} {} Prob: {:1.5f}'.format(word1, word2, word3, self.vocab[i], prob[i]))\n",
    "\n",
    "    def word_distance(self, word1, word2):\n",
    "        \"\"\"Compute the distance between the vector representations of two words.\"\"\"\n",
    "\n",
    "        if word1 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "        if word2 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "\n",
    "        idx1, idx2 = self.vocab.index(word1), self.vocab.index(word2)\n",
    "        word_rep1 = self.params.word_embedding_weights[idx1, :]\n",
    "        word_rep2 = self.params.word_embedding_weights[idx2, :]\n",
    "        diff = word_rep1 - word_rep2\n",
    "        return np.sqrt(np.sum(diff ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbwZCTkboEhz"
   },
   "source": [
    "To make your life easier, we have provided the routine *checking.check_gradients*, which checks your gradients using finite differences.\n",
    "You should make sure this check passes before continuing with the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5soRTiRn6W4"
   },
   "outputs": [],
   "source": [
    "def relative_error(a, b):\n",
    "    return np.abs(a - b) / (np.abs(a) + np.abs(b))\n",
    "\n",
    "\n",
    "def check_output_derivatives(model, input_batch, target_batch):\n",
    "    def softmax(z):\n",
    "        z = z.copy()\n",
    "        z -= z.max(1).reshape((-1, 1))\n",
    "        y = np.exp(z)\n",
    "        y /= y.sum(1).reshape((-1, 1))\n",
    "        return y\n",
    "\n",
    "    batch_size = input_batch.shape[0]\n",
    "    z = np.random.normal(size=(batch_size, model.vocab_size))\n",
    "    y = softmax(z)\n",
    "\n",
    "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "    loss_derivative = model.compute_loss_derivative(y, expanded_target_batch)\n",
    "\n",
    "    if loss_derivative is None:\n",
    "        print('Loss derivative not implemented yet.')\n",
    "        return False\n",
    "\n",
    "    if loss_derivative.shape != (batch_size, model.vocab_size):\n",
    "        print('Loss derivative should be size {} but is actually {}.'.format(\n",
    "            (batch_size, model.vocab_size), loss_derivative.shape))\n",
    "        return False\n",
    "\n",
    "    def obj(z):\n",
    "        y = softmax(z)\n",
    "        return model.compute_loss(y, expanded_target_batch)\n",
    "\n",
    "    for count in range(1000):\n",
    "        i, j = np.random.randint(0, loss_derivative.shape[0]), np.random.randint(0, loss_derivative.shape[1])\n",
    "\n",
    "        z_plus = z.copy()\n",
    "        z_plus[i, j] += EPS\n",
    "        obj_plus = obj(z_plus)\n",
    "\n",
    "        z_minus = z.copy()\n",
    "        z_minus[i, j] -= EPS\n",
    "        obj_minus = obj(z_minus)\n",
    "\n",
    "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
    "        rel = relative_error(empirical, loss_derivative[i, j])\n",
    "        if rel > 1e-4:\n",
    "            print('The loss derivative has a relative error of {}, which is too large.'.format(rel))\n",
    "            return False\n",
    "\n",
    "    print('The loss derivative looks OK.')\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_param_gradient(model, param_name, input_batch, target_batch):\n",
    "    activations = model.compute_activations(input_batch)\n",
    "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch)\n",
    "    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
    "\n",
    "    def obj(model):\n",
    "        activations = model.compute_activations(input_batch)\n",
    "        return model.compute_loss(activations.output_layer, expanded_target_batch)\n",
    "\n",
    "    dims = getattr(model.params, param_name).shape\n",
    "    is_matrix = (len(dims) == 2)\n",
    "\n",
    "    if getattr(param_gradient, param_name).shape != dims:\n",
    "        print('The gradient for {} should be size {} but is actually {}.'.format(\n",
    "            param_name, dims, getattr(param_gradient, param_name).shape))\n",
    "        return\n",
    "\n",
    "    for count in range(1000):\n",
    "        if is_matrix:\n",
    "            slc = np.random.randint(0, dims[0]), np.random.randint(0, dims[1])\n",
    "        else:\n",
    "            slc = np.random.randint(dims[0])\n",
    "\n",
    "        model_plus = model.copy()\n",
    "        getattr(model_plus.params, param_name)[slc] += EPS\n",
    "        obj_plus = obj(model_plus)\n",
    "\n",
    "        model_minus = model.copy()\n",
    "        getattr(model_minus.params, param_name)[slc] -= EPS\n",
    "        obj_minus = obj(model_minus)\n",
    "\n",
    "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
    "        exact = getattr(param_gradient, param_name)[slc]\n",
    "        rel = relative_error(empirical, exact)\n",
    "        if rel > 1e-4:\n",
    "            print('The loss derivative has a relative error of {}, which is too large.'.format(rel))\n",
    "            return False\n",
    "\n",
    "    print('The gradient for {} looks OK.'.format(param_name))\n",
    "\n",
    "\n",
    "def load_partially_trained_model():\n",
    "    obj = pickle.load(open(PARTIALLY_TRAINED_MODEL, 'rb'))\n",
    "    params = Params(obj['word_embedding_weights'], obj['embed_to_hid_weights'],\n",
    "                                   obj['hid_to_output_weights'], obj['hid_bias'],\n",
    "                                   obj['output_bias'])\n",
    "    vocab = obj['vocab']\n",
    "    return Model(params, vocab)\n",
    "\n",
    "\n",
    "def check_gradients():\n",
    "    \"\"\"Check the computed gradients using finite differences.\"\"\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "    np.seterr(all='ignore')  # suppress a warning which is harmless\n",
    "\n",
    "    model = load_partially_trained_model()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "    input_batch = train_inputs[:100, :]\n",
    "    target_batch = train_targets[:100]\n",
    "\n",
    "    if not check_output_derivatives(model, input_batch, target_batch):\n",
    "        return\n",
    "\n",
    "    for param_name in ['word_embedding_weights', 'embed_to_hid_weights', 'hid_to_output_weights',\n",
    "                       'hid_bias', 'output_bias']:\n",
    "        check_param_gradient(model, param_name, input_batch, target_batch)\n",
    "\n",
    "\n",
    "def print_gradients():\n",
    "    \"\"\"Print out certain derivatives for grading.\"\"\"\n",
    "\n",
    "    model = load_partially_trained_model()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "    input_batch = train_inputs[:100, :]\n",
    "    target_batch = train_targets[:100]\n",
    "\n",
    "    activations = model.compute_activations(input_batch)\n",
    "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch)\n",
    "    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
    "\n",
    "    print('loss_derivative[2, 5]', loss_derivative[2, 5])\n",
    "    print('loss_derivative[2, 121]', loss_derivative[2, 121])\n",
    "    print('loss_derivative[5, 33]', loss_derivative[5, 33])\n",
    "    print('loss_derivative[5, 31]', loss_derivative[5, 31])\n",
    "    print()\n",
    "    print('param_gradient.word_embedding_weights[27, 2]', param_gradient.word_embedding_weights[27, 2])\n",
    "    print('param_gradient.word_embedding_weights[43, 3]', param_gradient.word_embedding_weights[43, 3])\n",
    "    print('param_gradient.word_embedding_weights[22, 4]', param_gradient.word_embedding_weights[22, 4])\n",
    "    print('param_gradient.word_embedding_weights[2, 5]', param_gradient.word_embedding_weights[2, 5])\n",
    "    print()\n",
    "    print('param_gradient.embed_to_hid_weights[10, 2]', param_gradient.embed_to_hid_weights[10, 2])\n",
    "    print('param_gradient.embed_to_hid_weights[15, 3]', param_gradient.embed_to_hid_weights[15, 3])\n",
    "    print('param_gradient.embed_to_hid_weights[30, 9]', param_gradient.embed_to_hid_weights[30, 9])\n",
    "    print('param_gradient.embed_to_hid_weights[35, 21]', param_gradient.embed_to_hid_weights[35, 21])\n",
    "    print()\n",
    "    print('param_gradient.hid_bias[10]', param_gradient.hid_bias[10])\n",
    "    print('param_gradient.hid_bias[20]', param_gradient.hid_bias[20])\n",
    "    print()\n",
    "    print('param_gradient.output_bias[0]', param_gradient.output_bias[0])\n",
    "    print('param_gradient.output_bias[1]', param_gradient.output_bias[1])\n",
    "    print('param_gradient.output_bias[2]', param_gradient.output_bias[2])\n",
    "    print('param_gradient.output_bias[3]', param_gradient.output_bias[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TCLl7v189SI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss derivative looks OK.\n",
      "The gradient for word_embedding_weights looks OK.\n",
      "The gradient for embed_to_hid_weights looks OK.\n",
      "The gradient for hid_to_output_weights looks OK.\n",
      "The gradient for hid_bias looks OK.\n",
      "The gradient for output_bias looks OK.\n",
      "loss_derivative[2, 5] 0.001112231773782498\n",
      "loss_derivative[2, 121] -0.9991004720395987\n",
      "loss_derivative[5, 33] 0.0001903237803173703\n",
      "loss_derivative[5, 31] -0.7999757709589483\n",
      "\n",
      "param_gradient.word_embedding_weights[27, 2] -0.27199539981936866\n",
      "param_gradient.word_embedding_weights[43, 3] 0.8641722267354154\n",
      "param_gradient.word_embedding_weights[22, 4] -0.2546730202374648\n",
      "param_gradient.word_embedding_weights[2, 5] 0.0\n",
      "\n",
      "param_gradient.embed_to_hid_weights[10, 2] -0.6526990313918257\n",
      "param_gradient.embed_to_hid_weights[15, 3] -0.13106433000472612\n",
      "param_gradient.embed_to_hid_weights[30, 9] 0.11846774618169399\n",
      "param_gradient.embed_to_hid_weights[35, 21] -0.10004526104604386\n",
      "\n",
      "param_gradient.hid_bias[10] 0.2537663873815642\n",
      "param_gradient.hid_bias[20] -0.03326739163635357\n",
      "\n",
      "param_gradient.output_bias[0] -2.0627596032173052\n",
      "param_gradient.output_bias[1] 0.0390200857392169\n",
      "param_gradient.output_bias[2] -0.7561537928318482\n",
      "param_gradient.output_bias[3] 0.21235172051123635\n"
     ]
    }
   ],
   "source": [
    "check_gradients()\n",
    "print_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtC-br-N5xGT"
   },
   "source": [
    "Once you've implemented the gradient computation, you'll need to train the model.\n",
    "The function *train* implements the main training procedure.\n",
    "It takes two arguments:\n",
    "\n",
    "\n",
    "*   *embedding_dim*: The number of dimensions in the distributed representation.\n",
    "*   *num_hid*: The number of hidden units\n",
    "\n",
    "\n",
    "As the model trains, the script prints out some numbers that tell you how well the training is going.\n",
    "It shows:\n",
    "\n",
    "\n",
    "*   The cross entropy on the last 100 mini-batches of the training set. This is shown after every 100 mini-batches.\n",
    "*   The cross entropy on the entire validation set every 1000 mini-batches of training.\n",
    "\n",
    "At the end of training, this function shows the cross entropies on the training, validation and test sets.\n",
    "It will return a *Model* instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akBYJQOdLfaF"
   },
   "outputs": [],
   "source": [
    "_train_inputs = None\n",
    "_train_targets = None\n",
    "_vocab = None\n",
    "\n",
    "DEFAULT_TRAINING_CONFIG = {'batch_size': 100,  # the size of a mini-batch\n",
    "                           'learning_rate': 0.1,  # the learning rate\n",
    "                           'momentum': 0.9,  # the decay parameter for the momentum vector\n",
    "                           'epochs': 50,  # the maximum number of epochs to run\n",
    "                           'init_wt': 0.01,  # the standard deviation of the initial random weights\n",
    "                           'context_len': 3,  # the number of context words used\n",
    "                           'show_training_CE_after': 100,  # measure training error after this many mini-batches\n",
    "                           'show_validation_CE_after': 1000,  # measure validation error after this many mini-batches\n",
    "                           }\n",
    "\n",
    "\n",
    "def find_occurrences(word1, word2, word3):\n",
    "    \"\"\"Lists all the words that followed a given tri-gram in the training set and the number of\n",
    "    times each one followed it.\"\"\"\n",
    "\n",
    "    # cache the data so we don't keep reloading\n",
    "    global _train_inputs, _train_targets, _vocab\n",
    "    if _train_inputs is None:\n",
    "        data_obj = pickle.load(open(data_location, 'rb'))\n",
    "        _vocab = data_obj['vocab']\n",
    "        _train_inputs, _train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "\n",
    "    if word1 not in _vocab:\n",
    "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "    if word2 not in _vocab:\n",
    "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "    if word3 not in _vocab:\n",
    "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n",
    "\n",
    "    idx1, idx2, idx3 = _vocab.index(word1), _vocab.index(word2), _vocab.index(word3)\n",
    "    idxs = np.array([idx1, idx2, idx3])\n",
    "\n",
    "    matches = np.all(_train_inputs == idxs.reshape((1, -1)), 1)\n",
    "\n",
    "    if np.any(matches):\n",
    "        counts = collections.defaultdict(int)\n",
    "        for m in np.where(matches)[0]:\n",
    "            counts[_vocab[_train_targets[m]]] += 1\n",
    "\n",
    "        word_counts = sorted(list(counts.items()), key=lambda t: t[1], reverse=True)\n",
    "        print('The tri-gram \"{} {} {}\" was followed by the following words in the training set:'.format(\n",
    "            word1, word2, word3))\n",
    "        for word, count in word_counts:\n",
    "            if count > 1:\n",
    "                print('    {} ({} times)'.format(word, count))\n",
    "            else:\n",
    "                print('    {} (1 time)'.format(word))\n",
    "    else:\n",
    "        print('The tri-gram \"{} {} {}\" did not occur in the training set.'.format(word1, word2, word3))\n",
    "\n",
    "\n",
    "def train(embedding_dim, num_hid, config=DEFAULT_TRAINING_CONFIG):\n",
    "    \"\"\"This is the main training routine for the language model. It takes two parameters:\n",
    "\n",
    "        embedding_dim, the dimension of the embeddilanguage_model.pyng space\n",
    "        num_hid, the number of hidden units.\"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    vocab = data_obj['vocab']\n",
    "    train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "    valid_inputs, valid_targets = data_obj['valid_inputs'], data_obj['valid_targets']\n",
    "    test_inputs, test_targets = data_obj['test_inputs'], data_obj['test_targets']\n",
    "\n",
    "    # Randomly initialize the trainable parameters\n",
    "    model = Model.random_init(config['init_wt'], vocab, config['context_len'], embedding_dim, num_hid)\n",
    "\n",
    "    # Variables used for early stopping\n",
    "    best_valid_CE = np.infty\n",
    "    end_training = False\n",
    "\n",
    "    # Initialize the momentum vector to all zeros\n",
    "    delta = Params.zeros(len(vocab), config['context_len'], embedding_dim, num_hid)\n",
    "\n",
    "    this_chunk_CE = 0.\n",
    "    batch_count = 0\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        if end_training:\n",
    "            break\n",
    "\n",
    "        print()\n",
    "        print('Epoch', epoch)\n",
    "\n",
    "        for m, (input_batch, target_batch) in enumerate(get_batches(train_inputs, train_targets,\n",
    "                                                                    config['batch_size'])):\n",
    "            batch_count += 1\n",
    "\n",
    "            # Forward propagate\n",
    "            activations = model.compute_activations(input_batch)\n",
    "\n",
    "            # Compute loss derivative\n",
    "            expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "            loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch)\n",
    "            loss_derivative /= config['batch_size']\n",
    "\n",
    "            # Measure loss function\n",
    "            cross_entropy = model.compute_loss(activations.output_layer, expanded_target_batch) / config['batch_size']\n",
    "            this_chunk_CE += cross_entropy\n",
    "            if batch_count % config['show_training_CE_after'] == 0:\n",
    "                print('Batch {} Train CE {:1.3f}'.format(\n",
    "                    batch_count, this_chunk_CE / config['show_training_CE_after']))\n",
    "                this_chunk_CE = 0.\n",
    "\n",
    "            # Backpropagate\n",
    "            loss_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
    "\n",
    "            # Update the momentum vector and model parameters\n",
    "            delta = config['momentum'] * delta + loss_gradient\n",
    "            model.params -= config['learning_rate'] * delta\n",
    "\n",
    "            # Validate\n",
    "            if batch_count % config['show_validation_CE_after'] == 0:\n",
    "                print('Running validation...')\n",
    "                cross_entropy = model.evaluate(valid_inputs, valid_targets)\n",
    "                print('Validation cross-entropy: {:1.3f}'.format(cross_entropy))\n",
    "\n",
    "                if cross_entropy > best_valid_CE:\n",
    "                    print('Validation error increasing!  Training stopped.')\n",
    "                    end_training = True\n",
    "                    break\n",
    "\n",
    "                best_valid_CE = cross_entropy\n",
    "\n",
    "    print()\n",
    "    train_CE = model.evaluate(train_inputs, train_targets)\n",
    "    print('Final training cross-entropy: {:1.3f}'.format(train_CE))\n",
    "    valid_CE = model.evaluate(valid_inputs, valid_targets)\n",
    "    print('Final validation cross-entropy: {:1.3f}'.format(valid_CE))\n",
    "    test_CE = model.evaluate(test_inputs, test_targets)\n",
    "    print('Final test cross-entropy: {:1.3f}'.format(test_CE))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZX-g3K-F55h7"
   },
   "source": [
    "Run the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwlRG7j8LmIM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch 100 Train CE 4.528\n",
      "Batch 200 Train CE 4.464\n",
      "Batch 300 Train CE 4.464\n",
      "Batch 400 Train CE 4.426\n",
      "Batch 500 Train CE 4.454\n",
      "Batch 600 Train CE 4.434\n",
      "Batch 700 Train CE 4.418\n",
      "Batch 800 Train CE 4.429\n",
      "Batch 900 Train CE 4.413\n",
      "Batch 1000 Train CE 4.416\n",
      "Running validation...\n",
      "Validation cross-entropy: 4.399\n",
      "Batch 1100 Train CE 4.370\n",
      "Batch 1200 Train CE 4.311\n",
      "Batch 1300 Train CE 4.207\n",
      "Batch 1400 Train CE 4.176\n",
      "Batch 1500 Train CE 4.111\n",
      "Batch 1600 Train CE 4.086\n",
      "Batch 1700 Train CE 4.084\n",
      "Batch 1800 Train CE 4.019\n",
      "Batch 1900 Train CE 3.997\n",
      "Batch 2000 Train CE 3.999\n",
      "Running validation...\n",
      "Validation cross-entropy: 3.980\n",
      "Batch 2100 Train CE 3.978\n",
      "Batch 2200 Train CE 3.908\n",
      "Batch 2300 Train CE 3.900\n",
      "Batch 2400 Train CE 3.862\n",
      "Batch 2500 Train CE 3.848\n",
      "Batch 2600 Train CE 3.743\n",
      "Batch 2700 Train CE 3.721\n",
      "Batch 2800 Train CE 3.675\n",
      "Batch 2900 Train CE 3.589\n",
      "Batch 3000 Train CE 3.550\n",
      "Running validation...\n",
      "Validation cross-entropy: 3.560\n",
      "Batch 3100 Train CE 3.537\n",
      "Batch 3200 Train CE 3.519\n",
      "Batch 3300 Train CE 3.472\n",
      "Batch 3400 Train CE 3.407\n",
      "Batch 3500 Train CE 3.428\n",
      "Batch 3600 Train CE 3.383\n",
      "Batch 3700 Train CE 3.403\n",
      "\n",
      "Epoch 2\n",
      "Batch 3800 Train CE 3.346\n",
      "Batch 3900 Train CE 3.347\n",
      "Batch 4000 Train CE 3.300\n",
      "Running validation...\n",
      "Validation cross-entropy: 3.313\n",
      "Batch 4100 Train CE 3.315\n",
      "Batch 4200 Train CE 3.275\n",
      "Batch 4300 Train CE 3.273\n",
      "Batch 4400 Train CE 3.294\n",
      "Batch 4500 Train CE 3.291\n",
      "Batch 4600 Train CE 3.253\n",
      "Batch 4700 Train CE 3.215\n",
      "Batch 4800 Train CE 3.173\n",
      "Batch 4900 Train CE 3.196\n",
      "Batch 5000 Train CE 3.194\n",
      "Running validation...\n",
      "Validation cross-entropy: 3.212\n",
      "Batch 5100 Train CE 3.188\n",
      "Batch 5200 Train CE 3.194\n",
      "Batch 5300 Train CE 3.158\n",
      "Batch 5400 Train CE 3.153\n",
      "Batch 5500 Train CE 3.167\n",
      "Batch 5600 Train CE 3.144\n",
      "Batch 5700 Train CE 3.144\n",
      "Batch 5800 Train CE 3.142\n",
      "Batch 5900 Train CE 3.114\n",
      "Batch 6000 Train CE 3.091\n",
      "Running validation...\n",
      "Validation cross-entropy: 3.127\n",
      "Batch 6100 Train CE 3.095\n",
      "Batch 6200 Train CE 3.083\n",
      "Batch 6300 Train CE 3.078\n",
      "Batch 6400 Train CE 3.086\n",
      "Batch 6500 Train CE 3.107\n",
      "Batch 6600 Train CE 3.129\n",
      "Batch 6700 Train CE 3.064\n",
      "Batch 6800 Train CE 3.056\n",
      "Batch 6900 Train CE 3.062\n",
      "Batch 7000 Train CE 3.045\n",
      "Running validation...\n",
      "Validation cross-entropy: 3.045\n",
      "Batch 7100 Train CE 3.032\n",
      "Batch 7200 Train CE 3.062\n",
      "Batch 7300 Train CE 3.051\n",
      "Batch 7400 Train CE 3.028\n",
      "\n",
      "Epoch 3\n",
      "Batch 7500 Train CE 3.026\n",
      "Batch 7600 Train CE 2.985\n",
      "Batch 7700 Train CE 3.016\n",
      "Batch 7800 Train CE 2.997\n",
      "Batch 7900 Train CE 2.996\n",
      "Batch 8000 Train CE 2.969\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.996\n",
      "Batch 8100 Train CE 3.001\n",
      "Batch 8200 Train CE 2.972\n",
      "Batch 8300 Train CE 2.995\n",
      "Batch 8400 Train CE 2.970\n",
      "Batch 8500 Train CE 2.990\n",
      "Batch 8600 Train CE 2.959\n",
      "Batch 8700 Train CE 2.949\n",
      "Batch 8800 Train CE 2.947\n",
      "Batch 8900 Train CE 2.962\n",
      "Batch 9000 Train CE 2.947\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.965\n",
      "Batch 9100 Train CE 2.967\n",
      "Batch 9200 Train CE 2.917\n",
      "Batch 9300 Train CE 2.930\n",
      "Batch 9400 Train CE 2.935\n",
      "Batch 9500 Train CE 2.981\n",
      "Batch 9600 Train CE 2.921\n",
      "Batch 9700 Train CE 2.940\n",
      "Batch 9800 Train CE 2.913\n",
      "Batch 9900 Train CE 2.902\n",
      "Batch 10000 Train CE 2.903\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.924\n",
      "Batch 10100 Train CE 2.931\n",
      "Batch 10200 Train CE 2.898\n",
      "Batch 10300 Train CE 2.908\n",
      "Batch 10400 Train CE 2.914\n",
      "Batch 10500 Train CE 2.937\n",
      "Batch 10600 Train CE 2.923\n",
      "Batch 10700 Train CE 2.887\n",
      "Batch 10800 Train CE 2.906\n",
      "Batch 10900 Train CE 2.891\n",
      "Batch 11000 Train CE 2.900\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.888\n",
      "Batch 11100 Train CE 2.877\n",
      "\n",
      "Epoch 4\n",
      "Batch 11200 Train CE 2.855\n",
      "Batch 11300 Train CE 2.853\n",
      "Batch 11400 Train CE 2.855\n",
      "Batch 11500 Train CE 2.875\n",
      "Batch 11600 Train CE 2.893\n",
      "Batch 11700 Train CE 2.849\n",
      "Batch 11800 Train CE 2.890\n",
      "Batch 11900 Train CE 2.859\n",
      "Batch 12000 Train CE 2.832\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.884\n",
      "Batch 12100 Train CE 2.885\n",
      "Batch 12200 Train CE 2.819\n",
      "Batch 12300 Train CE 2.853\n",
      "Batch 12400 Train CE 2.837\n",
      "Batch 12500 Train CE 2.809\n",
      "Batch 12600 Train CE 2.857\n",
      "Batch 12700 Train CE 2.848\n",
      "Batch 12800 Train CE 2.857\n",
      "Batch 12900 Train CE 2.827\n",
      "Batch 13000 Train CE 2.811\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.835\n",
      "Batch 13100 Train CE 2.840\n",
      "Batch 13200 Train CE 2.846\n",
      "Batch 13300 Train CE 2.801\n",
      "Batch 13400 Train CE 2.787\n",
      "Batch 13500 Train CE 2.810\n",
      "Batch 13600 Train CE 2.865\n",
      "Batch 13700 Train CE 2.813\n",
      "Batch 13800 Train CE 2.826\n",
      "Batch 13900 Train CE 2.848\n",
      "Batch 14000 Train CE 2.783\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.828\n",
      "Batch 14100 Train CE 2.825\n",
      "Batch 14200 Train CE 2.802\n",
      "Batch 14300 Train CE 2.852\n",
      "Batch 14400 Train CE 2.788\n",
      "Batch 14500 Train CE 2.821\n",
      "Batch 14600 Train CE 2.812\n",
      "Batch 14700 Train CE 2.793\n",
      "Batch 14800 Train CE 2.834\n",
      "Batch 14900 Train CE 2.840\n",
      "\n",
      "Epoch 5\n",
      "Batch 15000 Train CE 2.796\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.803\n",
      "Batch 15100 Train CE 2.776\n",
      "Batch 15200 Train CE 2.745\n",
      "Batch 15300 Train CE 2.807\n",
      "Batch 15400 Train CE 2.758\n",
      "Batch 15500 Train CE 2.781\n",
      "Batch 15600 Train CE 2.788\n",
      "Batch 15700 Train CE 2.738\n",
      "Batch 15800 Train CE 2.781\n",
      "Batch 15900 Train CE 2.764\n",
      "Batch 16000 Train CE 2.761\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.796\n",
      "Batch 16100 Train CE 2.782\n",
      "Batch 16200 Train CE 2.764\n",
      "Batch 16300 Train CE 2.770\n",
      "Batch 16400 Train CE 2.790\n",
      "Batch 16500 Train CE 2.754\n",
      "Batch 16600 Train CE 2.766\n",
      "Batch 16700 Train CE 2.787\n",
      "Batch 16800 Train CE 2.759\n",
      "Batch 16900 Train CE 2.759\n",
      "Batch 17000 Train CE 2.771\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.791\n",
      "Batch 17100 Train CE 2.799\n",
      "Batch 17200 Train CE 2.765\n",
      "Batch 17300 Train CE 2.783\n",
      "Batch 17400 Train CE 2.758\n",
      "Batch 17500 Train CE 2.764\n",
      "Batch 17600 Train CE 2.768\n",
      "Batch 17700 Train CE 2.791\n",
      "Batch 17800 Train CE 2.784\n",
      "Batch 17900 Train CE 2.773\n",
      "Batch 18000 Train CE 2.788\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.775\n",
      "Batch 18100 Train CE 2.757\n",
      "Batch 18200 Train CE 2.736\n",
      "Batch 18300 Train CE 2.772\n",
      "Batch 18400 Train CE 2.764\n",
      "Batch 18500 Train CE 2.749\n",
      "Batch 18600 Train CE 2.727\n",
      "\n",
      "Epoch 6\n",
      "Batch 18700 Train CE 2.723\n",
      "Batch 18800 Train CE 2.718\n",
      "Batch 18900 Train CE 2.720\n",
      "Batch 19000 Train CE 2.749\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.773\n",
      "Batch 19100 Train CE 2.693\n",
      "Batch 19200 Train CE 2.728\n",
      "Batch 19300 Train CE 2.741\n",
      "Batch 19400 Train CE 2.722\n",
      "Batch 19500 Train CE 2.749\n",
      "Batch 19600 Train CE 2.756\n",
      "Batch 19700 Train CE 2.765\n",
      "Batch 19800 Train CE 2.746\n",
      "Batch 19900 Train CE 2.708\n",
      "Batch 20000 Train CE 2.748\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.752\n",
      "Batch 20100 Train CE 2.714\n",
      "Batch 20200 Train CE 2.730\n",
      "Batch 20300 Train CE 2.712\n",
      "Batch 20400 Train CE 2.730\n",
      "Batch 20500 Train CE 2.729\n",
      "Batch 20600 Train CE 2.718\n",
      "Batch 20700 Train CE 2.736\n",
      "Batch 20800 Train CE 2.751\n",
      "Batch 20900 Train CE 2.736\n",
      "Batch 21000 Train CE 2.729\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.750\n",
      "Batch 21100 Train CE 2.732\n",
      "Batch 21200 Train CE 2.732\n",
      "Batch 21300 Train CE 2.704\n",
      "Batch 21400 Train CE 2.718\n",
      "Batch 21500 Train CE 2.736\n",
      "Batch 21600 Train CE 2.714\n",
      "Batch 21700 Train CE 2.736\n",
      "Batch 21800 Train CE 2.674\n",
      "Batch 21900 Train CE 2.711\n",
      "Batch 22000 Train CE 2.693\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.738\n",
      "Batch 22100 Train CE 2.728\n",
      "Batch 22200 Train CE 2.733\n",
      "Batch 22300 Train CE 2.728\n",
      "\n",
      "Epoch 7\n",
      "Batch 22400 Train CE 2.738\n",
      "Batch 22500 Train CE 2.675\n",
      "Batch 22600 Train CE 2.687\n",
      "Batch 22700 Train CE 2.720\n",
      "Batch 22800 Train CE 2.742\n",
      "Batch 22900 Train CE 2.680\n",
      "Batch 23000 Train CE 2.692\n",
      "Running validation...\n",
      "Validation cross-entropy: 2.750\n",
      "Validation error increasing!  Training stopped.\n",
      "\n",
      "Final training cross-entropy: 2.699\n",
      "Final validation cross-entropy: 2.750\n",
      "Final test cross-entropy: 2.752\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "num_hid = 128\n",
    "trained_model = train(embedding_dim, num_hid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FD5Om0ypNPe"
   },
   "source": [
    "To convince us that you have correctly implemented the gradient computations, please include the following with your assignment submission:\n",
    "\n",
    "\n",
    "\n",
    "*   You will submit *language_model.ipynb* through MarkUs.\n",
    "You do not need to modify any of the code except the parts we asked you to implement.\n",
    "*   In your writeup, include the output of the function *print_gradients*.\n",
    "This prints out part of the gradients for a partially trained network which we have provided, and we will check them against the correct outputs. **Important:** make sure to give the output of *print_gradients*, not *check_gradients*.\n",
    "\n",
    "This is worth 4 points: 1 for the loss derivatives, 1 for the bias gradients, and 2 for the weight gradients.\n",
    "Since we gave you a gradient checker, you have no excuse for not getting full points on this part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-TThqObiw58"
   },
   "source": [
    "# Part 4: Analysis (4pts)\n",
    "\n",
    "In this part, you will analyze the representation learned by the network. You should first train a\n",
    "model with a 16-dimensional embedding and 128 hidden units, as discussed in the previous section;\n",
    "you’ll use this trained model for the remainder of this section.\n",
    "Important: if you’ve made any fixes\n",
    "to your gradient code, you must reload the language_model module and then re-run the training\n",
    "procedure.\n",
    "Python does not reload modules automatically, and you don’t want to accidentally\n",
    "analyze an old version of your model.\n",
    "\n",
    "These methods of the Model class can be used for analyzing the model after the training is\n",
    "done:\n",
    "\n",
    "\n",
    "\n",
    "*   *display_nearest_words* lists the words whose embedding vectors are nearest to the given\n",
    "word\n",
    "*   *word_distance* computes the distance between the embeddings of two words\n",
    "*   *predict_next_word* shows the possible next words the model considers most likely, along\n",
    "with their probabilities\n",
    "\n",
    "\n",
    "We also include:\n",
    "\n",
    "\n",
    "*    *tsne_plot_representation* creates a 2-dimensional embedding of the distributed representation space using\n",
    "an algorithm called t-SNE. (You don’t need to know what this is for the assignment, but we\n",
    "may cover it later in the course.) Nearby points in this 2-D space are meant to correspond to\n",
    "nearby points in the 16-D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1PCshvD3ijGM"
   },
   "outputs": [],
   "source": [
    "import numpy as Math\n",
    "\n",
    "def Hbeta(D=Math.array([]), beta=1.0):\n",
    "    \"\"\"Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution.\"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = Math.exp(-D.copy() * beta);\n",
    "    sumP = sum(P);\n",
    "    H = Math.log(sumP) + beta * Math.sum(D * P) / sumP;\n",
    "    P = P / sumP;\n",
    "    return H, P;\n",
    "\n",
    "\n",
    "def x2p(X=Math.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity.\"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape;\n",
    "    sum_X = Math.sum(Math.square(X), 1);\n",
    "    D = Math.add(Math.add(-2 * Math.dot(X, X.T), sum_X).T, sum_X);\n",
    "    P = Math.zeros((n, n));\n",
    "    beta = Math.ones((n, 1));\n",
    "    logU = Math.log(perplexity);\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point \", i, \" of \", n, \"...\")\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -Math.inf;\n",
    "        betamax = Math.inf;\n",
    "        Di = D[i, Math.concatenate((Math.r_[0:i], Math.r_[i + 1:n]))];\n",
    "        (H, thisP) = Hbeta(Di, beta[i]);\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU;\n",
    "        tries = 0;\n",
    "        while Math.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i];\n",
    "                if betamax == Math.inf or betamax == -Math.inf:\n",
    "                    beta[i] = beta[i] * 2;\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2;\n",
    "            else:\n",
    "                betamax = beta[i];\n",
    "                if betamin == Math.inf or betamin == -Math.inf:\n",
    "                    beta[i] = beta[i] / 2;\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2;\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i]);\n",
    "            Hdiff = H - logU;\n",
    "            tries = tries + 1;\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, Math.concatenate((Math.r_[0:i], Math.r_[i + 1:n]))] = thisP;\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: \", Math.mean(Math.sqrt(1 / beta)))\n",
    "    return P;\n",
    "\n",
    "\n",
    "def pca(X=Math.array([]), no_dims=50):\n",
    "    \"\"\"Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions.\"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape;\n",
    "    X = X - Math.tile(Math.mean(X, 0), (n, 1));\n",
    "    (l, M) = Math.linalg.eig(Math.dot(X.T, X));\n",
    "    Y = Math.dot(X, M[:, 0:no_dims]);\n",
    "    return Y;\n",
    "\n",
    "\n",
    "def tsne(X=Math.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions.\n",
    "    The syntaxis of the function is Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if X.dtype != \"float64\":\n",
    "        print(\"Error: array X should have type float64.\");\n",
    "        return -1;\n",
    "    # if no_dims.__class__ != \"<type 'int'>\":\t\t\t# doesn't work yet!\n",
    "    #\tprint(\"Error: number of dimensions should be an integer.\");\n",
    "    #\treturn -1;\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims);\n",
    "    (n, d) = X.shape;\n",
    "    max_iter = 1000;\n",
    "    initial_momentum = 0.5;\n",
    "    final_momentum = 0.8;\n",
    "    eta = 500;\n",
    "    min_gain = 0.01;\n",
    "    Y = Math.random.randn(n, no_dims);\n",
    "    dY = Math.zeros((n, no_dims));\n",
    "    iY = Math.zeros((n, no_dims));\n",
    "    gains = Math.ones((n, no_dims));\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity);\n",
    "    P = P + Math.transpose(P);\n",
    "    P = P / Math.sum(P);\n",
    "    P = P * 4;  # early exaggeration\n",
    "    P = Math.maximum(P, 1e-12);\n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = Math.sum(Math.square(Y), 1);\n",
    "        num = 1 / (1 + Math.add(Math.add(-2 * Math.dot(Y, Y.T), sum_Y).T, sum_Y));\n",
    "        num[range(n), range(n)] = 0;\n",
    "        Q = num / Math.sum(num);\n",
    "        Q = Math.maximum(Q, 1e-12);\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q;\n",
    "        for i in range(n):\n",
    "            dY[i, :] = Math.sum(Math.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0);\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0) != (iY > 0)) + (gains * 0.8) * ((dY > 0) == (iY > 0));\n",
    "        gains[gains < min_gain] = min_gain;\n",
    "        iY = momentum * iY - eta * (gains * dY);\n",
    "        Y = Y + iY;\n",
    "        Y = Y - Math.tile(Math.mean(Y, 0), (n, 1));\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            C = Math.sum(P * Math.log(P / Q));\n",
    "            print(\"Iteration \", (iter + 1), \": error is \", C)\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4;\n",
    "\n",
    "    # Return solution\n",
    "    return Y;\n",
    "\n",
    "def tsne_plot_representation(model, out_name=\"\"):\n",
    "    \"\"\"Plot a 2-D visualization of the learned representations using t-SNE.\"\"\"\n",
    "    print(model.params.word_embedding_weights.shape)\n",
    "    mapped_X = tsne(model.params.word_embedding_weights)\n",
    "    pylab.figure()\n",
    "    for i, w in enumerate(model.vocab):\n",
    "        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w, fontsize=5)\n",
    "    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n",
    "    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n",
    "#     pylab.show()\n",
    "    pylab.savefig(out_name)\n",
    "\n",
    "def tsne_plot_GLoVE_representation(W_final, b_final, out_name=\"\"):\n",
    "    \"\"\"Plot a 2-D visualization of the learned representations using t-SNE.\"\"\"\n",
    "    mapped_X = tsne(W_final)\n",
    "    pylab.figure()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    for i, w in enumerate(data_obj['vocab']):\n",
    "        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w, fontsize=5)\n",
    "    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n",
    "    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n",
    "#     pylab.show()\n",
    "    pylab.savefig(out_name)\n",
    "\n",
    "def plot_2d_GLoVE_representation(W_final, b_final, out_name=\"\"):\n",
    "    \"\"\"Plot a 2-D visualization of the learned representations.\"\"\"\n",
    "    mapped_X = W_final\n",
    "    pylab.figure()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    for i, w in enumerate(data_obj['vocab']):\n",
    "        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w, fontsize=5)\n",
    "    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n",
    "    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n",
    "#     pylab.show()\n",
    "    pylab.savefig(out_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZokGoNbivHp"
   },
   "source": [
    "Using these methods, please answer the following questions, each of which is worth 1 point.\n",
    "\n",
    "\n",
    "\n",
    "1.   Pick three words from the vocabulary that go well together (for example, ‘*government of united*’,\n",
    "‘*city of new*’, ‘*life in the*’, ‘*he is the*’ etc.).\n",
    "Use the model to predict the next word.\n",
    "Does the model give sensible predictions?\n",
    "Try to find an example where it makes a plausible prediction even though the 4-gram wasn’t present in the dataset (*raw_sentences.txt*).\n",
    "To help you out, the function *find_occurrences* lists the words that appear after a given 3-gram in the training set.\n",
    "2.   Plot the 2-dimensional visualization using the method *tsne_plot_representation*.\n",
    "Look at the plot and find a few clusters of related words.\n",
    "What do the words in each cluster have in common?\n",
    "Plot the 2-dimensional visualization using the method *tsne_plot_GLoVE_representation* for a 256 dimensional embedding.\n",
    "How do the t-SNE embeddings for both models compare?\n",
    "Plot the 2-dimensional visualization using the method *plot_2d_GLoVE_representation*.\n",
    "How does this compare to the t-SNE embeddings?\n",
    "(You don’t need to include the plots with your submission.)\n",
    "\n",
    "3.   Are the words ‘*new*’ and ‘*york*’ close together in the learned representation?\n",
    "Why or why not?\n",
    "4.   Which pair of words is closer together in the learned representation: (‘*government*’, ‘*political*’), or (‘*government*’, ‘*university*’)?\n",
    "Why do you think this is?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZjEVB1oirWq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_plot_representation(trained_model, \"representation.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gJQE9Gxsftn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_plot_GLoVE_representation(W_final, b_final, \"glove.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pw5WKPyntY_O"
   },
   "outputs": [],
   "source": [
    "plot_2d_GLoVE_representation(W_final_2d, b_final_2d, \"2d_glove.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MR6DEZvM_0jg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_plot_GLoVE_representation(W_final_2d, b_final_2d, \"glove_2d.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_model.predict_next_word('where', 'might', 'you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_occurrences('where', 'might', 'you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.word_distance('university', 'government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.word_distance('political', 'government')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3RzXTTzsdy9"
   },
   "source": [
    "# What you have to submit\n",
    "\n",
    "For reference, here is everything you need to hand in. See the top of this handout for submission\n",
    "directions.\n",
    "\n",
    "\n",
    "\n",
    "*   A PDF file titled *a1-writeup.pdf* containing the following:\n",
    "Both questions from Part 2, the output of *print_gradients()*, answers to all four questions from Part 3.\n",
    "*   Your code file *language_model.ipynb*\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
