\documentclass{myhw}
\linespread{1.05}        % Palatino needs more leading (space between lines)
\usepackage{extarrows}
\usepackage{mathrsfs}
\usepackage{braket}
\titleformat{\section}[runin]{\sffamily\bfseries}{}{}{}[]
\titleformat{\subsection}[runin]{\sffamily\bfseries}{}{}{}[]
\renewcommand{\exname}{Question }
\renewcommand{\subexcounter}{(\alph{homeworkSectionCounter})}
\newcommand{\id}{\text{Id}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\rib}{\text{Rib}}

\title{CSC 2515 Homework 5}
\begin{document}

%% Question 1
\begin{homeworkProblem}
\textbf{EM for Probabilistic PCA} 
%%% Subquestion 1
\begin{homeworkSection}	
\emph{E-step. Calculate the statistics of the posterior
distribution $q(z)=p(z|\textbf{x})$ which youâ€™ll need for the M-step.} \\
\\
From the Appendix, we know how to get the distribution of $z$ given $\textbf{x}$, where $z$ is drawn from Gaussian distribution and $\textbf{x}$ is drawn from a spherical Gaussian distribution. \\
In our setting, 
\begin{gather*}
\begin{aligned}
p(z) &= \mathcal{N}(z|0,1) \\
p(\textbf{x}|z) &= \mathcal{N}(\textbf{x}|z\textbf{u},\sigma^2\textbf{I})
\end{aligned}
\end{gather*}
To apply the parameters in the formulae of the Appendix, we have
\begin{gather*}
\begin{aligned}
& \bm{\mu} = 0, \bm{\Sigma} = 1, \\
& \bm{A} = \bm{u}, \bm{B} = 0, \bm{S} = \sigma^2\bm{I} \\
& \bm{C} = (1+\bm{u}^T(\sigma^2)^{-1}\bm{u})^{-1} 
= \frac{\sigma^2}{\sigma^2 + \bm{u}^T\bm{u}} 
\end{aligned}
\end{gather*}
Thus, we can obtain the folowing formulae:
\begin{gather*}
\begin{aligned}
p(\bm{x}) &= \mathcal{N}(\bm{x}|0, \bm{u}^T \bm{u} + \sigma^2) \\
p(z|\bm{x}) &= \mathcal{N}(z|\bm{C}(\bm{u}^T(\sigma^2)^{-1}\bm{x}),\bm{C}) \\
&= \mathcal{N}(z|\frac{\bm{u}^T\bm{x}}{\sigma^2+\bm{u}^T\bm{u}},
\frac{\sigma^2}{\sigma^2+\bm{u}^T\bm{u}})
\end{aligned}
\end{gather*}
As a result, 
\begin{gather*}
\begin{aligned}
m=E[z|\bm{x}] &= \frac{\bm{u}^T\bm{x}}{\sigma^2+\bm{u}^T\bm{u}} \\
Var[z|\bm{x}] &= \frac{\sigma^2}{\sigma^2+\bm{u}^T\bm{u}} \\
s=E[z^2|\bm{x}] &= Var[z|\bm{x}] + E[z|\bm{x}]^2 \\
&= \frac{\sigma^4+\sigma^2\bm{u}^T\bm{u}+(\bm{u}^T\bm{x})^2}{(\sigma^2+\bm{u}^T\bm{u})^2}
\end{aligned}
\end{gather*}
\end{homeworkSection}
%%% Subquestion 2
\begin{homeworkSection}	
\emph{M-step. Re-estimate the parameters, which consist of the vector $\bm{u}$. derive a formula for $\bm{u}_{new}$ that maximizes the expected log-likelihood, i.e.,} 
\begin{gather*}
\bm{u}_{new} = \arg\max_{\bm{u}} \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{q(z^{(i)})} 
[\log p(z^{(i)}, \bm{x}^{(i)})]
\end{gather*}
Denote the function to be maximized as 
\begin{gather*}
\begin{aligned}
\mathbb{F} &= \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{q(z^{(i)})} 
[\log p(z^{(i)}, \bm{x}^{(i)})] \\
&= \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{q(z^{(i)})} 
[\log q(z^{(i)})p(\bm{x}^{(i)})] \\
\end{aligned}
\end{gather*}
Then, 
\begin{gather*}
\begin{aligned}
\log p(\bm{x}^{(i)})q(z^{(i)}) &= \log 
\frac{1}{\sqrt{2\pi (\bm{u}^T\bm{u}+\sigma^2)}} e^{-\frac{\bm{x}^{(i)2}}{2(\bm{u}^T\bm{u}+\sigma^2)}}
\frac{1}{\sqrt{2\pi \frac{\sigma^2}{\bm{u}^T\bm{u}+\sigma^2}}} e^{-\frac{(z^{(i)} - \frac{\bm{u}^T\bm{x}^{(i)}}{\sigma^2+\bm{u}^T\bm{u}})^2}{2 \frac{\sigma^2}{\bm{u}^T\bm{u}+\sigma^2} }} \\
&\propto -\frac{\bm{x}^{(i)2}}{2(\bm{u}^T\bm{u}+\sigma^2)} -\frac{(z^{(i)} - \frac{\bm{u}^T\bm{x}^{(i)}}{\sigma^2+\bm{u}^T\bm{u}})^2}{2 \frac{\sigma^2}{\bm{u}^T\bm{u}+\sigma^2} } \\
&\propto -\frac{\bm{x}^{(i)2} \sigma^2 + [\bm{u}^T\bm{x}^{(i)}-(\sigma^2+\bm{u}^T\bm{u})z^{(i)}]^2}{2\sigma^2 (\bm{u}^T\bm{u}+\sigma^2)} \\
&\propto -\frac{z^{(i)2}(\sigma^2+\bm{u}^T\bm{u})}{2\sigma^2} 
+ \frac{z^{(i)}\bm{u}^T\bm{x}^{(i)}}{\sigma^2} \\
&\propto -\frac{z^{(i)2} \bm{u}^T\bm{u} }{2\sigma^2} 
+ \frac{z^{(i)}\bm{u}^T\bm{x}^{(i)}}{\sigma^2}
\end{aligned}
\end{gather*}
Apply the liearity of expectation, 
%\begin{gather*}
%\begin{aligned}
%\mathbb{F} &= \frac{1}{N} \sum_{i=1}^N \mathbb{E}
%[\log p(\bm{x}^{(i)})q(z^{(i)})] \\
%&= \frac{1}{N} \sum_{i=1}^N [-\frac{\mathbb{E} [z^{(i)2}|\bm{x}^{(i)}] \bm{u}^T\bm{u}}{2\sigma^2} 
%+ \frac{\mathbb{E} [z^{(i)}|\bm{x}^{(i)}] \bm{u}^T \bm{x}^{(i)} }{\sigma^2}] \\
%&= \frac{1}{N} \sum_{i=1}^N [-\frac{ m \bm{u}^T\bm{u}}{2\sigma^2} 
%+ \frac{ s \bm{u}^T\bm{x}^{(i)}}{\sigma^2}] \\
%&= -\frac{ m \bm{u}^T\bm{u}}{2\sigma^2} + 
%\frac{ s \frac{1}{N} \sum_{i=1}^N \bm{u}^T \bm{x}^{(i)} }{\sigma^2}
%\end{aligned}
%\end{gather*}
\begin{gather*}
\begin{aligned}
\mathbb{F} &= \frac{1}{N} \sum_{i=1}^N \mathbb{E}
[\log p(\bm{x}^{(i)})q(z^{(i)})] \\
&= \frac{1}{N} \sum_{i=1}^N [-\frac{\mathbb{E} [z^{(i)2}|\bm{x}^{(i)}] \bm{u}^T\bm{u}}{2\sigma^2} 
+ \frac{\mathbb{E} [z^{(i)}|\bm{x}^{(i)}] \bm{u}^T \bm{x}^{(i)} }{\sigma^2}] \\
&= \frac{1}{N} \sum_{i=1}^N [-\frac{ s^{(i)} \bm{u}^T\bm{u}}{2\sigma^2} 
+ \frac{ m^{(i)} \bm{u}^T\bm{x}^{(i)}}{\sigma^2}] \\
\end{aligned}
\end{gather*}
To get the gradient with repect to $\bm{u}$,
\begin{gather*}
\frac{\partial \mathbb{F}}{\partial \bm{u}} = 
-\frac{1}{N} \sum_{i=1}^N [\frac{s^{(i)} \bm{u}}{\sigma^2} + \frac{m^{(i)} \bm{x}^{(i)}}{\sigma^2}] = 0 \\
\bm{u} \leftarrow \frac{\frac{1}{N} \sum_{i=1}^N m^{(i)} \bm{x}^{(i)}}{\frac{1}{N} \sum_{i=1}^N s^{(i)}} \\
\bm{u} \leftarrow \frac{\sum_{i=1}^N m^{(i)} \bm{x}^{(i)}}{\sum_{i=1}^N s^{(i)}}
\end{gather*}
\end{homeworkSection}
\end{homeworkProblem}


%% Question 2
\begin{homeworkProblem}
\textbf{Contraction Maps}
%%% Subquestion 1
\begin{homeworkSection}
\emph{Show that the Bellman backup operator $T^\pi$ is a contraction map in the $||\cdot||_\infty$ norm.} \\
\\
Our claim is that the Bellman backup  operator $T^\pi$ is a contraction map, which means
\begin{gather*}
\begin{aligned}
||T^\pi Q_1 - T^\pi Q_2||_\infty \leq \gamma||Q_1 - Q_2||_\infty \\
\end{aligned}
\end{gather*}
By applying Bellman equation:
\begin{gather*}
\begin{aligned}
Q_{k+1}(s,a) \leftarrow r(s,a) + \gamma \sum_{s'} P(s'|a,s) \sum_{a'} \pi(a'|s') Q_k(s', a')
\end{aligned}
\end{gather*}
we have 
\begin{gather*}
\begin{aligned}
& |T^\pi Q_1 (s,a) - T^\pi Q_2 (s,a)|_\infty \\
=& |[r(s,a) + \gamma \sum_{s'} P(s'|a,s) \sum_{a'} \pi(a'|s') Q_1(s', a')] - 
[r(s,a) + \gamma \sum_{s'} P(s'|a,s) \sum_{a'} \pi(a'|s') Q_2(s', a')]|_\infty \\
=& \gamma|\sum_{s'} P(s'|a,s) \sum_{a'} \pi(a'|s') [Q_1(s', a') - Q_2(s', a')]|_\infty \\
\leq& \gamma\sum_{s'} P(s'|a,s) \sum_{a'} \pi(a'|s') |Q_1(s', a') - Q_2(s', a')|_\infty \\
\leq& \gamma |Q_1(s', a') - Q_2(s', a')|_\infty \sum_{s'} P(s'|a,s) \sum_{a'} \pi(a'|s') \\
=& \gamma |Q_1(s', a') - Q_2(s', a')|_\infty
\end{aligned}
\end{gather*}
This is true for any $(s, a)$, so 
\begin{gather*}
\begin{aligned}
||T^\pi Q_1 - T^\pi Q_2||_\infty \leq \gamma||Q_1 - Q_2||_\infty \\
\end{aligned}
\end{gather*}
which is what we wanted to show.
\end{homeworkSection}
\end{homeworkProblem}


%% Question 3
\begin{homeworkProblem}
\textbf{Q-Learning}
%%% Subquestion 1
\begin{homeworkSection}	
\emph{Determine the optimal policy and the Q-function for the optimal policy.} \\
\\
The optimal policy will be 
\begin{gather*}
\begin{aligned}
\pi(Stay|s_1) = 0; 
\pi(Switch|s_1) = 1; 
\pi(Stay|s_2) = 1; 
\pi(Switch|s_2) = 2; 
\end{aligned}
\end{gather*}
Then, by applying Bellman equation
\begin{gather*}
\begin{aligned}
Q^*(s,a) = r(s,a) + \gamma \sum_{s'} P(s'|a,s) \max_{a'} Q^*(s',a')
\end{aligned}
\end{gather*}
, we get
\begin{gather*}
\begin{aligned}
Q^*(s_1,Stay) = R(s_1) + 0.9 \max_{a'} Q^*(s_1,a') \\
Q^*(s_1,Switch) = R(s_1) + 0.9 \max_{a'} Q^*(s_2,a') \\
Q^*(s_2,Stay) = R(s_2) + 0.9 \max_{a'} Q^*(s_2,a') \\
Q^*(s_2,Switch) = R(s_2) + 0.9 \max_{a'} Q^*(s_1,a') 
\end{aligned}
\end{gather*}
Applying to this question's setting, 
\begin{gather*}
\begin{aligned}
R(s_1) + 0.9 \max_{a'\in\mathcal{A}} Q^*(s_1,a') - Q^*(s_1, Stay) &= 0 \\
R(s_1) + 0.9 \max_{a'\in\mathcal{A}} Q^*(s_2,a') - Q^*(s_1, Switch) &= 0 \\
R(s_2) + 0.9 \max_{a'\in\mathcal{A}} Q^*(s_2,a') - Q^*(s_2, Stay) &= 0 \\
R(s_2) + 0.9 \max_{a'\in\mathcal{A}} Q^*(s_1,a') - Q^*(s_2, Switch) &= 0 \\
\end{aligned}
\end{gather*}
that is,
\begin{gather*}
\begin{aligned}
1 + 0.9 \max \{Q^*(s_1,Stay),Q^*(s_1,Switch)\} - Q^*(s_1, Stay) &= 0 \\
1 + 0.9 \max \{Q^*(s_2,Stay),Q^*(s_2,Switch)\} - Q^*(s_1, Switch) &= 0 \\
2 + 0.9 \max \{Q^*(s_2,Stay),Q^*(s_2,Switch)\} - Q^*(s_2, Stay) &= 0 \\
2 + 0.9 \max \{Q^*(s_1,Stay),Q^*(s_1,Switch)\} - Q^*(s_2, Switch) &= 0 \\
\end{aligned}
\end{gather*}
so
\begin{gather*}
\begin{aligned}
Q^*(s_2,Stay) &= 1 + Q^*(s_1,Switch) \\
Q^*(s_2,Switch) &= 1 + Q^*(s_1,Stay) \\
\end{aligned}
\end{gather*}
Apply them,
\begin{gather*}
\begin{aligned}
1 + 0.9 \max \{Q^*(s_1,Stay),Q^*(s_1,Switch)\} - Q^*(s_1, Stay) &= 0 \\
1.9 + 0.9 \max \{Q^*(s_1,Stay),Q^*(s_1,Switch)\} - Q^*(s_1, Switch) &= 0 \\
\end{aligned}
\end{gather*}
so
\begin{gather*}
\begin{aligned}
Q^*(s_1,Switch) &= Q^*(s_1,Stay) + 0.9 \\ 
Q^*(s_2,Stay) &= Q^*(s_1,Stay) + 1.9 \\ 
Q^*(s_2,Switch) &= Q^*(s_1,Stay) + 1 \\ 
\end{aligned}
\end{gather*}
Finally, we have
\begin{gather*}
\begin{aligned}
Q^*(s_1,Stay) &= 18.1 \\
Q^*(s_1,Switch) &= Q^*(s_1,Stay) + 0.9 = 19 \\ 
Q^*(s_2,Stay) &= Q^*(s_1,Stay) + 1.9 = 20 \\ 
Q^*(s_2,Switch) &= Q^*(s_1,Stay) + 1 = 19.1 \\ 
\end{aligned}
\end{gather*}
or
\begin{center}
\begin{tabular}{c|c|c}
  & Stay & Switch \\ 
 \hline
 $s_1$ & 18.1 & 19 \\  
 \hline
 $s_2$ & 20 & 19.1    
\end{tabular}
\end{center}
\end{homeworkSection}
%%% Subquestion 2
\begin{homeworkSection}	
\emph{Now suppose we apply Q-learning, except that instead of the $\epsilon$-greedy policy, the agent follows the greedy policy which always chooses $\pi(s) = \arg \max_a Q(s,a)$. Assume the agent starts in state $S_0 = s_1$. Give an example of a Q-function that is in equilibrium (i.e. it will never change after the Q-learning update rule is applied), but which results in a suboptimal policy.} \\
\\
To be in equilibrium, the expected change in $Q(S, A)$ should be zero, i.e.
\begin{gather*}
\begin{aligned}
\mathbb{E} [R + \gamma \max_{a'\in\mathcal{A}} Q(S',a') - Q(S,A)|S,A] = 0
\end{aligned}
\end{gather*}
%It also causes another problem. \\
According to Q-Learning, we should initialize $Q(s,a)$ for all the $(s,a) \in \mathcal{S} \times \mathcal{A}$. \\
We could assign them as 10 and 0's, where the Q-function is shown as 
\begin{center}
\begin{tabular}{c|c|c}
  & Stay & Switch \\ 
 \hline
 $s_1$ & 10 & 0 \\  
 \hline
 $s_2$ & 0 & 0    
\end{tabular} 
\end{center} 
For the time step $t=0$, \\
Choose $A_t$ according to the greedy policy, i.e., 
$A_0 \leftarrow \arg\max_{a\in\mathcal{A}}Q(S_0, a)=\arg\max_{a\in\mathcal{A}}Q(s_1, a)$ \\
Because $Q(s_1,Stay)=10$ and $Q(s_1,Switch)  = 0$, we will choose $A_0 = Stay$. \\
Then, the new state becomes $S_1=s_1$, as we choose to \emph{Stay}. \\
Also, $R_0 = r(s_1,Stay)=1$. \\
Finally, we update the action-value function at state-action $(s_1,Stay)$ as 
\begin{gather*}
\begin{aligned}
Q(s_1,Stay) &\leftarrow Q(s_1,Stay) + \alpha [R_0 + \gamma \max_{a'\in\mathcal{A}}Q(s_1,a')-Q(s_1,Stay)] \\
&= 10 + \alpha [1 + 0.9 \times 10 - 10] \\
&= 10
\end{aligned}
\end{gather*}
The table of Q-function still is 
\begin{center}
\begin{tabular}{c|c|c}
  & Stay & Switch \\ 
 \hline
 $s_1$ & 10 & 0 \\  
 \hline
 $s_2$ & 0 & 0    
\end{tabular} 
\end{center}
Continue the algorithm, we will find it choose \emph{Stay} all the time and will never visit $s_2$. \\
And the expected change in $Q(S, A)$ should be zero, i.e.
\begin{gather*}
\begin{aligned}
\mathbb{E} [R(s_1) + \gamma \max_{a'\in\mathcal{A}} Q(s_1,a') - Q(s_1,Stay)|s_1,Stay] = 0
\end{aligned}
\end{gather*}
It is an equilibrium situation, but never converging to the optimal policy.
\end{homeworkSection}
\end{homeworkProblem}


\end{document}

%\begin{gather*}
%\end{gather*}

%\begin{gather*}
%\begin{aligned}
%\end{aligned}
%\end{gather*}





