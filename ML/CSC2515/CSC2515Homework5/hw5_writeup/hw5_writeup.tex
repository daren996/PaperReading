\documentclass{myhw}
\linespread{1.05}        % Palatino needs more leading (space between lines)
\usepackage{extarrows}
\usepackage{mathrsfs}
\usepackage{braket}
\titleformat{\section}[runin]{\sffamily\bfseries}{}{}{}[]
\titleformat{\subsection}[runin]{\sffamily\bfseries}{}{}{}[]
\renewcommand{\exname}{Question }
\renewcommand{\subexcounter}{(\alph{homeworkSectionCounter})}
\newcommand{\id}{\text{Id}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\rib}{\text{Rib}}

\title{CSC 2515 Homework 5}
\begin{document}

%% Question 1
\begin{homeworkProblem}
\textbf{EM for Probabilistic PCA} 
%%% Subquestion 1
\begin{homeworkSection}	
\emph{E-step. Calculate the statistics of the posterior
distribution $q(z)=p(z|\textbf{x})$ which youâ€™ll need for the M-step.} \\
\\
From the Appendix, we know how to get the distribution of $z$ given $\textbf{x}$, where $z$ is drawn from Gaussian distribution and $\textbf{x}$ is drawn from a spherical Gaussian distribution. \\
In our setting, 
\begin{gather*}
\begin{aligned}
p(z) &= \mathcal{N}(z|0,1) \\
p(\textbf{x}|z) &= \mathcal{N}(\textbf{x}|z\textbf{u},\sigma^2\textbf{I})
\end{aligned}
\end{gather*}
To apply the parameters in the formulae of the Appendix, we have
\begin{gather*}
\begin{aligned}
& \bm{\mu} = 0, \bm{\Sigma} = 1, \\
& \bm{A} = \bm{u}, \bm{B} = 0, \bm{S} = \sigma^2\bm{I} \\
& \bm{C} = (1+\bm{u}^T(\sigma^2)^{-1}\bm{u})^{-1} 
= \frac{\sigma^2}{\sigma^2 + \bm{u}^T\bm{u}} 
\end{aligned}
\end{gather*}
Thus, we can obtain the folowing formulae:
\begin{gather*}
\begin{aligned}
p(\bm{x}) &= \mathcal{N}(\bm{x}|0, \bm{u}^T \bm{u} + \sigma^2) \\
p(z|\bm{x}) &= \mathcal{N}(z|\bm{C}(\bm{u}^T(\sigma^2)^{-1}\bm{x}),\bm{C}) \\
&= \mathcal{N}(z|\frac{\bm{u}^T\bm{x}}{\sigma^2+\bm{u}^T\bm{u}},
\frac{\sigma^2}{\sigma^2+\bm{u}^T\bm{u}})
\end{aligned}
\end{gather*}
As a result, 
\begin{gather*}
\begin{aligned}
m=E[z|\bm{x}] &= \frac{\bm{u}^T\bm{x}}{\sigma^2+\bm{u}^T\bm{u}} \\
Var[z|\bm{x}] &= \frac{\sigma^2}{\sigma^2+\bm{u}^T\bm{u}} \\
s=E[z^2|\bm{x}] &= Var[z|\bm{x}] + E[z|\bm{x}]^2 \\
&= \frac{\sigma^4+\sigma^2\bm{u}^T\bm{u}+(\bm{u}^T\bm{x})^2}{(\sigma^2+\bm{u}^T\bm{u})^2}
\end{aligned}
\end{gather*}
\end{homeworkSection}
%%% Subquestion 2
\begin{homeworkSection}	
\emph{M-step. Re-estimate the parameters, which consist of the vector $\bm{u}$. derive a formula for $\bm{u}_{new}$ that maximizes the expected log-likelihood, i.e.,} 
\begin{gather*}
\bm{u}_{new} = \arg\max_{\bm{u}} \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{q(z^{(i)})} 
[\log p(z^{(i)}, \bm{x}^{(i)})]
\end{gather*}
Denote the function to be maximized as 
\begin{gather*}
\begin{aligned}
\mathbb{F} &= \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{q(z^{(i)})} 
[\log p(z^{(i)}, \bm{x}^{(i)})] \\
&= \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{q(z^{(i)})} 
[\log q(z^{(i)})p(\bm{x}^{(i)})] \\
\end{aligned}
\end{gather*}
Then, 
\begin{gather*}
\begin{aligned}
\log p(\bm{x}^{(i)})q(z^{(i)}) &= \log 
\frac{1}{\sqrt{2\pi (\bm{u}^T\bm{u}+\sigma^2)}} e^{-\frac{\bm{x}^{(i)2}}{2(\bm{u}^T\bm{u}+\sigma^2)}}
\frac{1}{\sqrt{2\pi \frac{\sigma^2}{\bm{u}^T\bm{u}+\sigma^2}}} e^{-\frac{(z^{(i)} - \frac{\bm{u}^T\bm{x}^{(i)}}{\sigma^2+\bm{u}^T\bm{u}})^2}{2 \frac{\sigma^2}{\bm{u}^T\bm{u}+\sigma^2} }} \\
&\propto -\frac{\bm{x}^{(i)2}}{2(\bm{u}^T\bm{u}+\sigma^2)} -\frac{(z^{(i)} - \frac{\bm{u}^T\bm{x}^{(i)}}{\sigma^2+\bm{u}^T\bm{u}})^2}{2 \frac{\sigma^2}{\bm{u}^T\bm{u}+\sigma^2} } \\
&\propto -\frac{\bm{x}^{(i)2} \sigma^2 + [\bm{u}^T\bm{x}^{(i)}-(\sigma^2+\bm{u}^T\bm{u})z^{(i)}]^2}{2\sigma^2 (\bm{u}^T\bm{u}+\sigma^2)} \\
&\propto -\frac{z^{(i)2}(\sigma^2+\bm{u}^T\bm{u})}{2\sigma^2} 
+ \frac{z^{(i)}\bm{u}^T\bm{x}^{(i)}}{\sigma^2} \\
&\propto -\frac{z^{(i)2} \bm{u}^T\bm{u} }{2\sigma^2} 
+ \frac{z^{(i)}\bm{u}^T\bm{x}^{(i)}}{\sigma^2}
\end{aligned}
\end{gather*}
Apply the liearity of expectation, 
%\begin{gather*}
%\begin{aligned}
%\mathbb{F} &= \frac{1}{N} \sum_{i=1}^N \mathbb{E}
%[\log p(\bm{x}^{(i)})q(z^{(i)})] \\
%&= \frac{1}{N} \sum_{i=1}^N [-\frac{\mathbb{E} [z^{(i)2}|\bm{x}^{(i)}] \bm{u}^T\bm{u}}{2\sigma^2} 
%+ \frac{\mathbb{E} [z^{(i)}|\bm{x}^{(i)}] \bm{u}^T \bm{x}^{(i)} }{\sigma^2}] \\
%&= \frac{1}{N} \sum_{i=1}^N [-\frac{ m \bm{u}^T\bm{u}}{2\sigma^2} 
%+ \frac{ s \bm{u}^T\bm{x}^{(i)}}{\sigma^2}] \\
%&= -\frac{ m \bm{u}^T\bm{u}}{2\sigma^2} + 
%\frac{ s \frac{1}{N} \sum_{i=1}^N \bm{u}^T \bm{x}^{(i)} }{\sigma^2}
%\end{aligned}
%\end{gather*}
\begin{gather*}
\begin{aligned}
\mathbb{F} &= \frac{1}{N} \sum_{i=1}^N \mathbb{E}
[\log p(\bm{x}^{(i)})q(z^{(i)})] \\
&= \frac{1}{N} \sum_{i=1}^N [-\frac{\mathbb{E} [z^{(i)2}|\bm{x}^{(i)}] \bm{u}^T\bm{u}}{2\sigma^2} 
+ \frac{\mathbb{E} [z^{(i)}|\bm{x}^{(i)}] \bm{u}^T \bm{x}^{(i)} }{\sigma^2}] \\
&= \frac{1}{N} \sum_{i=1}^N [-\frac{ m^{(i)} \bm{u}^T\bm{u}}{2\sigma^2} 
+ \frac{ s^{(i)} \bm{u}^T\bm{x}^{(i)}}{\sigma^2}] \\
\end{aligned}
\end{gather*}
To get the gradient with repect to $\bm{u}$,
\begin{gather*}
\frac{\partial \mathbb{F}}{\partial \bm{u}} = 
-\frac{1}{N} \sum_{i=1}^N [\frac{m^{(i)} \bm{u}}{\sigma^2} + \frac{s^{(i)} \bm{x}^{(i)} \bm{I}}{\sigma^2}] = 0 \\
\bm{u} \leftarrow \frac{\frac{1}{N} \sum_{i=1}^N s^{(i)} \bm{x}^{(i)} \bm{I} }{\frac{1}{N} \sum_{i=1}^N m^{(i)}} \\
\bm{u} \leftarrow \frac{\sum_{i=1}^N s^{(i)} \bm{x}^{(i)} \bm{I} }{\sum_{i=1}^N m^{(i)}}
\end{gather*}
\end{homeworkSection}
\end{homeworkProblem}


%% Question 2
\begin{homeworkProblem}
\textbf{Contraction Maps}
%%% Subquestion 1
\begin{homeworkSection}
\emph{} \\
\\
\end{homeworkSection}
\end{homeworkProblem}


%% Question 3
\begin{homeworkProblem}
\textbf{Q-Learning}
%%% Subquestion 1
\begin{homeworkSection}	
\emph{} \\
\\
\end{homeworkSection}
%%% Subquestion 2
\begin{homeworkSection}	
\emph{} \\
\\
\end{homeworkSection}
\end{homeworkProblem}


\end{document}

%\begin{gather*}
%\end{gather*}

%\begin{gather*}
%\begin{aligned}
%\end{aligned}
%\end{gather*}





