\documentclass{myhw}
\linespread{1.05}        % Palatino needs more leading (space between lines)
\usepackage{extarrows}
\usepackage{mathrsfs}
\usepackage{braket}
\titleformat{\section}[runin]{\sffamily\bfseries}{}{}{}[]
\titleformat{\subsection}[runin]{\sffamily\bfseries}{}{}{}[]
\renewcommand{\exname}{Question }
\renewcommand{\subexcounter}{(\alph{homeworkSectionCounter})}
\newcommand{\id}{\text{Id}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\rib}{\text{Rib}}

\title{CSC 2515 Homework 2}
\begin{document}

%% Question 1
\begin{homeworkProblem}
\textbf{Robust Regression.}
%%% Subquestion 1
\begin{homeworkSection}
\emph{Sketch the Huber loss $L_\delta(y,t)$ and squared error loss $L_{SE}(y,t) = \frac{1}{2}(y - t)^2$ for $t = 0$, either by hand or using a plotting library. Based on your sketch, why would you expect the Huber loss to be more robust to outliers? }\\
\\
When $t=2$, 
\begin{gather*}
L_{SE}(y,t) = \frac{1}{2}(y - t)^2 = \frac{1}{2}y^2 \\
L_\delta(y,t) = H_\delta(y-t) = \left\{ 
	\begin{array}{lr} 
	\frac{1}{2}y^2 & if\ |a| \le \delta \\ 
	\delta \times (|y|-\frac{1}{2}\delta) & if\ |a| > \delta
	\end{array} \right.
\end{gather*}
With the help of matplotlib, their sketches are shown in Figure \ref{fig:q1.1}. 
\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{q1.png} 
  \caption{Sketches of squared error loss and Huber loss function. }
  \label{fig:q1.1}
\end{figure}
\\
As you can see from the Figure \ref{fig:q1.1}, for LSE functions, when the value $|y|$ is large (which is also the case with outliers), $L_{SE}(y,t)$ is very sensitive to $|y|$ (and outliers). 
On the contrary, for Huber loss function, when $|y|$ exceeds $\delta$, $L_\delta(y, t)$ increases linearly to $|y|$, which shows that it is relatively less sensitive to outliers.
\end{homeworkSection}
%%% Subquestion 2
\begin{homeworkSection}
%Just as with linear regression, assume a linear model: $y=\textbf{w}^T\textbf{x}+b$.
\emph{Give formulas for the partial derivatives $\partial L_\delta / \partial \textbf{w}$ and $\partial L_\delta / \partial b$. }\\
\\
The formula for the derivative $H_\delta '(a)$ is
\begin{gather*}
H_\delta'(a) = 
	\left\{ 
	\begin{array}{lr} 
	a & if\ |a| \le \delta \\ 
	\delta & if\ a > \delta \\
	- \delta & if\ a < -\delta
	\end{array} \right.
\end{gather*}
Then, in terms of $a=y-t$, 
\begin{gather*}
H_\delta'(y-t) = 
	\left\{ 
	\begin{array}{lr} 
	y-t & if\ |y-t| \le \delta \\ 
	\delta & if\ y > t + \delta \\
	- \delta & if\ y < t - \delta
	\end{array} \right.
\end{gather*}
As for the linear model, \\if $\textbf{x}$ and $\textbf{w}$ are both $D$ dimensional vectors. 
\begin{gather*}
y = \textbf{w}^T\textbf{x} + b = \sum_j{w_j x_j + b} \\
\begin{aligned}
H_\delta'(y-t) = \frac{\partial H_\delta}{\partial y} & = 
	\left\{ 
	\begin{array}{lr} 
	\sum_j{w_j x_j + b} - t & if\ |y-t| \le \delta \\ 
	\delta & if\ y > t + \delta \\
	- \delta & if\ y < t - \delta
	\end{array} \right.
\\
\frac{\partial L_\delta}{\partial w_j} = \frac{\partial H_\delta}{\partial w_j} = 
\frac{\partial H_\delta}{\partial y} \frac{\partial y}{\partial w_j} & = 
	\left\{ 
	\begin{array}{lr} 
	x_j (\sum_{j'}{w_{j'} x_{j'} + b} - t) & if\ |y-t| \le \delta \\ 
	\delta x_j & if\ y > t + \delta \\
	- \delta x_j & if\ y < t - \delta
	\end{array} \right.
\\
\frac{\partial L_\delta}{\partial b} = \frac{\partial H_\delta}{\partial b} =
\frac{\partial H_\delta}{\partial y^{(i)}} \frac{\partial y^{(i)}}{\partial b} & = 
	\left\{ 
	\begin{array}{lr} 
	\sum_{j}{w_{j} x_{j} + b - t} & if\ |y-t| \le \delta \\ 
	\delta & if\ y > t + \delta \\
	- \delta & if\ y < t - \delta
	\end{array} \right.
\\
\frac{\partial L_\delta}{\partial \textbf{w}} & = 
	\left\{ 
	\begin{array}{lr} 
	\textbf{x}(\textbf{x}^T\textbf{w} + b - t) & if\ - \delta \textbf{1} \le \textbf{y}-\textbf{t} \le \delta \textbf{1} \\ 
	\delta \textbf{x} & if\ \textbf{y} > \textbf{t} + \delta \textbf{1} \\
	- \delta \textbf{x} & if\ \textbf{y} < \textbf{t} - \delta \textbf{1} \\
	\end{array} \right.
\\
\end{aligned}
\end{gather*}
When $\textbf{X}$ is a $N \times D$ matrix and $\textbf{w}$ is a $D$ dimensional vector. 
Then, 
\begin{gather*}
\textbf{y}=\textbf{X}\textbf{w}+b\textbf{1}, \\ y^{(i)} = \sum_j{w_j x_j^{(i)} + b}
\end{gather*}
We define $\mathscr{E}$ as the model's cost function.
\begin{gather*}
\mathscr{E}_\delta = \frac{1}{N}\sum_{i=1}^N{L_\delta} = \frac{1}{N}\sum_{i=1}^N{H_\delta}
\end{gather*}
We apply $y^{(i)}$ in the partial derivatives, 
\begin{gather*}
\begin{aligned}
\frac{\partial H_\delta}{\partial y^{(i)}} & = 
	\left\{ 
	\begin{array}{lr} 
	\sum_j{w_j x_j^{(i)} + b} - t^{(i)} & if\ |y^{(i)}-t^{(i)}| \le \delta \\ 
	\delta & if\ y^{(i)} > t^{(i)} + \delta \\
	- \delta & if\ y^{(i)} < t^{(i)} - \delta
	\end{array} \right.
\\
\frac{\partial \mathscr{E}_\delta}{\partial w_j} = 
\frac{\partial \mathscr{E}_\delta}{\partial y^{(i)}} \frac{\partial y^{(i)}}{\partial w_j} & = 
	\frac{1}{N} \sum_{i=1}^N
	\left\{ 
	\begin{array}{lr} 
	{x_j^{(i)} (\sum_{j'}{w_{j'} x_{j'}^{(i)} + b} - t^{(i)})} & if\ |y^{(i)}-t^{(i)}| \le \delta \\ 
	{\delta x_j^{(i)}} & if\ y^{(i)} > t^{(i)} + \delta \\
	- {\delta x_j^{(i)}} & if\ y^{(i)} < t^{(i)} - \delta
	\end{array} \right.
\\
\frac{\partial \mathscr{E}_\delta}{\partial b} = 
\frac{\partial \mathscr{E}_\delta}{\partial y^{(i)}} \frac{\partial y^{(i)}}{\partial b} & = 
	\frac{1}{N} \sum_{i=1}^N
	\left\{ 
	\begin{array}{lr} 
	(\sum_{j}{w_{j} x_{j}^{(i)} + b} - t^{(i)}) & if\ |y^{(i)}-t^{(i)}| \le \delta \\ 
	\delta & if\ y^{(i)} > t^{(i)} + \delta \\
	- \delta & if\ y^{(i)} < t^{(i)} - \delta
	\end{array} \right.
\end{aligned}
\end{gather*}
The partial derivatives can also be represented by matrix-vector format, 
\begin{gather*}
\begin{aligned}
\frac{\partial \mathscr{E}_\delta}{\partial \textbf{w}} & = 
	\left\{ 
	\begin{array}{lr} 
	\frac{1}{N} \textbf{X}^T(\textbf{Xw} + b\textbf{1} - \textbf{t}) & if\ - \delta \textbf{1} \le \textbf{y}-\textbf{t} \le \delta \textbf{1} \\ 
	\frac{\delta}{N} \textbf{X}^T \textbf{1} & if\ \textbf{y} > \textbf{t} + \delta \textbf{1} \\
	- \frac{\delta}{N} \textbf{X}^T \textbf{1} & if\ \textbf{y} < \textbf{t} - \delta \textbf{1}
	\end{array} \right.
\end{aligned}
\end{gather*}
\end{homeworkSection}
%%% Subquestion 3
\begin{homeworkSection}
\emph{Write Python code to perform (full batch mode) gradient descent on this model. }\\
\\
The update rule of gradient descent method is as follows, 
\begin{gather*}
w_j \leftarrow w_j - \alpha \frac{\partial \mathscr{E}_\delta}{\partial w_j} \\
\textbf{w} \leftarrow \textbf{w} - \alpha \frac{\partial \mathscr{E}_\delta}{\partial \textbf{w}} 
\end{gather*}
Without using \textbf{for} loop, we applied the \textbf{np.where} function to realize the gradient descent on the Huber loss function. 
First, we generalized three vectors using $\textbf{y}-\textbf{t}$, $\delta \textbf{1}$ and $-\delta \textbf{1}$ separately; 
then we used \textbf{np.where} to determine which line of the three vectors should be chosen.
\\
I sampled 50000 samples through $y=3x_1 + 5x_2 - 9x_3 + 28x_4 -10x_5 + 1$. Through 1000 iterations, the loss dropped to 0.001384. The predicted w was $[3.000181, 5.00041, -9.00045, 28.000627, -10.000461, 1.000123]$. However, I evaluated the efficiency of the algorithms with Huber loss function or square error loss function, and found that they took 6.6 and 0.24 seconds, respectively. I guessed the \textbf{np.where} function takes up most of the total time.
\\
\\
Code is in q1.py.
\end{homeworkSection}
\end{homeworkProblem}


%% Question 2
\begin{homeworkProblem}
\textbf{Locally Weighted Regression.}
%%% Subquestion 1
\begin{homeworkSection}
\emph{Show the solution to the weighted least squares problem. }\\
\\
We define $\mathscr{E}$ as the function of the weighted least squares problem.
\begin{gather*}
\begin{aligned}
\mathscr{E} & = \frac{1}{2} \sum_{i=1}^N{a^{(i)} (y^{(i)} - \textbf{w}^T \textbf{x}^{(i)})^2} + \frac{\lambda}{2} ||\textbf{w}||^2 \\
& = \frac{1}{2} (\textbf{y} - \textbf{Xw})^T \textbf{A} (\textbf{y} - \textbf{Xw}) + \frac{\lambda}{2} \textbf{w}^T \textbf{w} \\
& = \frac{1}{2} [(\textbf{y}^T \textbf{A} - \textbf{w}^T \textbf{X}^T \textbf{A})(\textbf{y} - \textbf{Xw})] + \frac{\lambda}{2} \textbf{w}^T \textbf{w} \\
& = \frac{1}{2} (\textbf{y}^T \textbf{A} \textbf{y} - \textbf{y}^T \textbf{AXw} - \textbf{w}^T \textbf{X}^T \textbf{Ay} + \textbf{w}^T \textbf{X}^T \textbf{AXw}) + \frac{\lambda}{2} \textbf{w}^T \textbf{w} \\
& = \frac{1}{2} [\textbf{y}^T \textbf{A} \textbf{y} - 2 \textbf{w}^T \textbf{X}^T \textbf{Ay} + (\textbf{Xw})^T \textbf{AXw}] + \frac{\lambda}{2} \textbf{w}^T \textbf{w} \\
\end{aligned}
\end{gather*}
To get solution for the problem, $\textbf{w}^*$, we need partial derivatives to zero.
\begin{gather*}
\begin{aligned}
\frac{\partial \mathscr{E}}{\partial \textbf{w}} = -\textbf{X}^T \textbf{Ay} + \textbf{X}^T \textbf{AXw} + \lambda \textbf{w} & = 0 \\
(\textbf{X}^T \textbf{AX} + \lambda \textbf{I}) \textbf{w} & = \textbf{X}^T \textbf{Ay} \\
\textbf{w} & = (\textbf{X}^T \textbf{AX} - \lambda \textbf{I})^{-1} \textbf{X}^T \textbf{Ay}
\end{aligned}
\end{gather*}
%\begin{gather*}
%\begin{aligned}
%\frac{\partial \mathscr{E}}{\partial w_j} = \sum_{i=1}^N{a^{(i)} (y^{(i)} - \textbf{w}^T \textbf{x}^{(i)})x_j^{(i)}} + \lambda w_j & = 0 \\
%\sum_{i=1}^N{a^{(i)} y^{(i)} x_j^{(i)}} - \sum_{i=1}^N{a^{(i)} x_j^{(i)} \textbf{w}^T \textbf{x}^{(i)}} + \lambda w_j & = 0 \\
%\sum_{i=1}^N{a^{(i)} x_j^{(i)} \textbf{w}^T \textbf{x}^{(i)}} - \lambda w_j & = \sum_{i=1}^N{a^{(i)} y^{(i)} x_j^{(i)}} \\
%\textbf{X}^T \textbf{AXw} - \lambda \textbf{1w} & = \textbf{X}^T \textbf{Ay} \\
%\textbf{w} & = (\textbf{X}^T \textbf{AX} - \lambda \textbf{1})^{-1} \textbf{X}^T \textbf{Ay}
%\end{aligned}
%\end{gather*}
\end{homeworkSection}
%%% Subquestion 2
\begin{homeworkSection}
\emph{Complete the implementation of locally reweighted least squares by providing the missing parts for q2.py. }\\
\\
See q2.py.
\end{homeworkSection}
%%% Subquestion 3
\begin{homeworkSection}
\emph{Based on our understanding of overfitting and underfitting, how would you expect the training error and the validation error to vary as a function of $\tau$? (I.e., what do you expect the curves to look like?) \\
Run the experiment. }\\
\\
I think the curve of both of them falls first and then rises. 
The curve of validation set is above the curve of training set.
Too little $\tau$ or too much $\tau$ is underfitting for both training set and validation set.
However, they may reach the lowest point at different $\tau$s, because when the train set curve reaches the lowest point, it may have been overfitting, and the lowest point of validation set needs to be found near this value. \\
\\
%If the $\tau$ is very small, the positive weights $A_{ii}$ will often become 0, which makes it impossible to calculate the invertion of $\textbf{X}^T\textbf{AX}+\lambda \textbf{I}$. 
%As a result, in my experiment, $\tau$ is between $10^{1.5}$ and $10^3$.
%\\
We've drawn the training and validation losses as a function of $\tau$ (using a log scale for $\tau$, which is between $10^{1}$ and $10^3$), as shown in Figure \ref{fig:q2.2}. \\
The picture is not exactly what I guesses.
As can be seen in the Figure \ref{fig:q2.2}, the smaller the $\tau$ is, the better the result of training set is, but the result of the validation set is not the best, which indicates that the model has been overfitted when $\tau$ is small.
When $\tau$ is around $10^2$, the results of validation set are optimized, and then the losses of both sets kept increasing, which indicates that the model is underfitting then.
\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{q2.png} 
  \caption{The losses of the training and validation dataset. }
  \label{fig:q2.2}
\end{figure}
\end{homeworkSection}
\end{homeworkProblem}


%% Question 3
\begin{homeworkProblem}
\textbf{AdaBoost.} \\
\emph{The goal of this question is to show that the AdaBoost algorithm changes the weights in order to force the weak learner to focus on difficult data points.} \\
\emph{Show that the error w.r.t. $(w_1',...,w_N')$ is exactly $\frac{1}{2}$. That is, show that}
\begin{gather*}
err_t' = \frac{\sum_{i=1}^N w_i' \mathbb{I} \{h_t(\textbf{x}^{(i)}) \neq t^{(i)} \} }{\sum_{i-1}^{N} w_i'} = \frac{1}{2}.
\end{gather*}
\\
Firstly, we deducted the $e^{2\alpha}$, 
\begin{gather*}
\begin{aligned}
e^{2\alpha} = \frac{1 - err_t}{err_t} 
& = \frac{\sum_{i=1}^N w_i \mathbb{I} \{h_t(\textbf{x}^{(i)}) = t^{(i)} \} }{\sum_{i-1}^{N} w_i} \frac{\sum_{i-1}^{N} w_i}{\sum_{i=1}^N w_i \mathbb{I} \{h_t(\textbf{x}^{(i)}) \neq t^{(i)} \} } \\
& = \frac{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i }{\sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i }
\end{aligned}
\end{gather*}
Apply the deductions of $w_i'$ and $e^{2\alpha}$, 
\begin{gather*}
\begin{aligned}
err_t' & = \frac{\sum_{i=1}^N w_i' \mathbb{I} \{h_t(\textbf{x}^{(i)}) \neq t^{(i)} \} }{\sum_{i-1}^{N} w_i'} \\
& = \frac{\sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i e^{\alpha_t}}{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i e^{-\alpha_t} + \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i e^{\alpha_t}} \\
& = \frac{\sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i e^{2\alpha_t} }{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i + \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i e^{2\alpha_t}} \\
& = \frac{\sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i \frac{1 - err_t}{err_t} }{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i + \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i \frac{1 - err_t}{err_t} } \\
& = \frac{\frac{1 - err_t}{err_t} \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i }{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i + \frac{1 - err_t}{err_t}  \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i } \\
& = \frac{\frac{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i }{\sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i } \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i }{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i + \frac{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i }{\sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i }  \sum_{i \in N, t^i \neq h_t(x^{(i)})} w_i } \\
& = \frac{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i}{\sum_{i \in N, t^i = h_t(x^{(i)})} w_i + \sum_{i \in N, t^i = h_t(x^{(i)})} w_i} \\
& = \frac{1}{2}
\end{aligned}
\end{gather*}

\end{homeworkProblem}
\end{document}

%\begin{gather*}
%\end{gather*}







